{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# query transformer according to blip"
      ],
      "metadata": {
        "id": "mAqJJ-zXtxOq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "KT_dYtkXs0Sw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchEmbed(nn.Module):\n",
        "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels, embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size\n",
        "        )\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerEncoderLayer(nn.Module):\n",
        "    def __init__(self, embed_dim=768, num_heads=8, ff_hidden_dim=2048, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout, batch_first=True)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(embed_dim, ff_hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(ff_hidden_dim, embed_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.self_attn(x, x, x)\n",
        "        x = x + self.dropout1(attn_output)\n",
        "        x = self.norm1(x)\n",
        "\n",
        "        ff_output = self.ff(x)\n",
        "        x = x + self.dropout2(ff_output)\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "class QFormer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 img_size=224,\n",
        "                 patch_size=16,\n",
        "                 in_channels=3,\n",
        "                 embed_dim=768,\n",
        "                 depth=6,\n",
        "                 num_heads=8,\n",
        "                 ff_hidden_dim=2048):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbed(img_size, patch_size, in_channels, embed_dim)\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(embed_dim, num_heads, ff_hidden_dim)\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, self.patch_embed.num_patches, embed_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.pos_embed\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = QFormer(img_size=224, patch_size=16, depth=4)\n",
        "dummy_input = torch.randn(2, 3, 224, 224)\n",
        "output = model(dummy_input)"
      ],
      "metadata": {
        "id": "VlcmgiO0vLAy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, (3, 224, 224))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBca0ZNFwcTq",
        "outputId": "0f045b0c-3939-4afd-b8f2-97fe8285953d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------\n",
            "        Layer (type)               Output Shape         Param #\n",
            "================================================================\n",
            "            Conv2d-1          [-1, 768, 14, 14]         590,592\n",
            "        PatchEmbed-2             [-1, 196, 768]               0\n",
            "MultiheadAttention-3  [[-1, 196, 768], [-1, 196, 196]]               0\n",
            "           Dropout-4             [-1, 196, 768]               0\n",
            "         LayerNorm-5             [-1, 196, 768]           1,536\n",
            "            Linear-6            [-1, 196, 2048]       1,574,912\n",
            "              GELU-7            [-1, 196, 2048]               0\n",
            "           Dropout-8            [-1, 196, 2048]               0\n",
            "            Linear-9             [-1, 196, 768]       1,573,632\n",
            "          Dropout-10             [-1, 196, 768]               0\n",
            "          Dropout-11             [-1, 196, 768]               0\n",
            "        LayerNorm-12             [-1, 196, 768]           1,536\n",
            "TransformerEncoderLayer-13             [-1, 196, 768]               0\n",
            "MultiheadAttention-14  [[-1, 196, 768], [-1, 196, 196]]               0\n",
            "          Dropout-15             [-1, 196, 768]               0\n",
            "        LayerNorm-16             [-1, 196, 768]           1,536\n",
            "           Linear-17            [-1, 196, 2048]       1,574,912\n",
            "             GELU-18            [-1, 196, 2048]               0\n",
            "          Dropout-19            [-1, 196, 2048]               0\n",
            "           Linear-20             [-1, 196, 768]       1,573,632\n",
            "          Dropout-21             [-1, 196, 768]               0\n",
            "          Dropout-22             [-1, 196, 768]               0\n",
            "        LayerNorm-23             [-1, 196, 768]           1,536\n",
            "TransformerEncoderLayer-24             [-1, 196, 768]               0\n",
            "MultiheadAttention-25  [[-1, 196, 768], [-1, 196, 196]]               0\n",
            "          Dropout-26             [-1, 196, 768]               0\n",
            "        LayerNorm-27             [-1, 196, 768]           1,536\n",
            "           Linear-28            [-1, 196, 2048]       1,574,912\n",
            "             GELU-29            [-1, 196, 2048]               0\n",
            "          Dropout-30            [-1, 196, 2048]               0\n",
            "           Linear-31             [-1, 196, 768]       1,573,632\n",
            "          Dropout-32             [-1, 196, 768]               0\n",
            "          Dropout-33             [-1, 196, 768]               0\n",
            "        LayerNorm-34             [-1, 196, 768]           1,536\n",
            "TransformerEncoderLayer-35             [-1, 196, 768]               0\n",
            "MultiheadAttention-36  [[-1, 196, 768], [-1, 196, 196]]               0\n",
            "          Dropout-37             [-1, 196, 768]               0\n",
            "        LayerNorm-38             [-1, 196, 768]           1,536\n",
            "           Linear-39            [-1, 196, 2048]       1,574,912\n",
            "             GELU-40            [-1, 196, 2048]               0\n",
            "          Dropout-41            [-1, 196, 2048]               0\n",
            "           Linear-42             [-1, 196, 768]       1,573,632\n",
            "          Dropout-43             [-1, 196, 768]               0\n",
            "          Dropout-44             [-1, 196, 768]               0\n",
            "        LayerNorm-45             [-1, 196, 768]           1,536\n",
            "TransformerEncoderLayer-46             [-1, 196, 768]               0\n",
            "================================================================\n",
            "Total params: 13,197,056\n",
            "Trainable params: 13,197,056\n",
            "Non-trainable params: 0\n",
            "----------------------------------------------------------------\n",
            "Input size (MB): 0.57\n",
            "Forward/backward pass size (MB): 176402.30\n",
            "Params size (MB): 50.34\n",
            "Estimated Total Size (MB): 176453.21\n",
            "----------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# multiple QFormers"
      ],
      "metadata": {
        "id": "NJJLwJzhyuzB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class MultiQFormer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoders=3,\n",
        "                 img_size=224,\n",
        "                 patch_size=16,\n",
        "                 in_channels=3,\n",
        "                 embed_dim=768,\n",
        "                 depth=6,\n",
        "                 num_heads=8,\n",
        "                 ff_hidden_dim=2048,\n",
        "                 output_dim=1024):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoders = nn.ModuleList([\n",
        "            QFormer(\n",
        "                img_size=img_size,\n",
        "                patch_size=patch_size,\n",
        "                in_channels=in_channels,\n",
        "                embed_dim=embed_dim,\n",
        "                depth=depth,\n",
        "                num_heads=num_heads,\n",
        "                ff_hidden_dim=ff_hidden_dim\n",
        "            )\n",
        "            for _ in range(num_encoders)\n",
        "        ])\n",
        "\n",
        "        self.output_proj = nn.Linear(num_encoders * embed_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        feats= []\n",
        "        for i, img in enumerate(x):\n",
        "            feat = self.encoders[i](img)\n",
        "            pooled = feat.mean(dim=1)\n",
        "            feats.append(pooled)\n",
        "\n",
        "        concat = torch.cat(feats, dim=-1)\n",
        "\n",
        "        return self.output_proj(concat)"
      ],
      "metadata": {
        "id": "FGl71xpPxpBP"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MultiQFormer(num_encoders=4, output_dim=1024)\n",
        "images = [torch.randn(1, 3, 224, 224) for _ in range(4)]\n",
        "output = model(images)"
      ],
      "metadata": {
        "id": "1BuyWbAdxqO8"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchsummary import summary\n",
        "summary(model, images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "JXo9gFTmy83O",
        "outputId": "b800b66b-6622-4e35-c6fc-e5084779ada3"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "rand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-3265864422>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchsummary\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# batch_size of 2 for batchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;31m# print(type(x[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchsummary/torchsummary.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# batch_size of 2 for batchnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0min_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0min_size\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0;31m# print(type(x[0]))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: rand(): argument 'size' failed to unpack the object at pos 2 with error \"type must be tuple of ints,but got Tensor\""
          ]
        }
      ]
    }
  ]
}