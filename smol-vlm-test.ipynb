{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce83dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U num2words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f33766",
   "metadata": {},
   "source": [
    "# DriveLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d99a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "drive_lm = load_dataset(\"MehdiJmlkh/DriveLM\")\n",
    "drive_lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57a7a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuscenes = load_dataset(\"MehdiJmlkh/nuscenes\")\n",
    "nuscenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ac142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "class DriveLM:\n",
    "    def __init__(self, drive_lm, nuscenes):\n",
    "        self.drive_lm = drive_lm\n",
    "        self.nuscenes = nuscenes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      sample = self.drive_lm[idx]\n",
    "      nuscenes_index = sample[\"nuscenes_index\"]\n",
    "      sample['images'] = self.nuscenes[nuscenes_index]\n",
    "\n",
    "      return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drive_lm)\n",
    "\n",
    "dataset = {\n",
    "    \"test\": DriveLM(drive_lm[\"test\"], nuscenes[\"test\"])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd9c7f0",
   "metadata": {},
   "source": [
    "# SmolVLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cd9fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import num2words\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, AutoImageProcessor\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from torchvision.transforms import functional as F_transforms\n",
    "from huggingface_hub import PyTorchModelHubMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9c534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "processor.image_processor.max_image_size[\"longest_edge\"]= 384\n",
    "processor.image_processor.do_image_splitting=False\n",
    "processor.image_processor.do_resize=True\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a253ed",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cc03db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VLMQADataset(Dataset):\n",
    "    def __init__(self, dataset: DriveLM, is_train=True):\n",
    "        self.dataset = dataset\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        user_msg = []\n",
    "        cameras = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']\n",
    "        for camera in cameras:\n",
    "            user_msg.extend([\n",
    "                {\"type\": \"text\", \"text\": camera},\n",
    "                {\"type\": \"image\", \"image\": item[\"images\"][camera]}\n",
    "            ])\n",
    "\n",
    "        user_msg.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\":f\"Scene description:{item['scene_description']} Question: {item['question']}\"\n",
    "        })\n",
    "\n",
    "        assistant_msg = [{\"type\": \"text\", \"text\": item[\"answer\"]}]\n",
    "\n",
    "        if not self.is_train:\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_msg\n",
    "                }\n",
    "            ]\n",
    "            return self.__apply_chat_template(conversation, True)\n",
    "\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_msg\n",
    "            }\n",
    "        ]\n",
    "        inputs = self.__apply_chat_template(conversation)\n",
    "\n",
    "        label_start_idx = self.__get_label_start_idx(inputs)\n",
    "\n",
    "        labels = inputs['input_ids'].clone()\n",
    "        labels[:, :label_start_idx] = -100\n",
    "        inputs['labels'] = labels\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def __apply_chat_template(self, conversation, add_generation_prompt=False):\n",
    "        return processor.apply_chat_template(\n",
    "            conversation,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __get_label_start_idx(self, inputs):\n",
    "        utterance_id = processor.tokenizer.convert_tokens_to_ids('<end_of_utterance>')\n",
    "        utterance_idx = inputs['input_ids'][0].tolist().index(utterance_id)\n",
    "        num_assistant_ids = 4\n",
    "        label_idx = utterance_idx + num_assistant_ids + 1\n",
    "\n",
    "        return label_idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56765a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = VLMQADataset(dataset[\"test\"], is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98f3433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_sample_and_output(index, model):\n",
    "    sample = dataset[\"test\"][index]\n",
    "\n",
    "    def display_image(image, title):\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    cameras = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']\n",
    "    for i, camera in enumerate(cameras):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        display_image(sample['images'][camera], camera)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for key, value in sample.items():\n",
    "        if type(value) is str:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "    output = model.generate(**test_dataset[index].to(\"cuda\", dtype=torch.bfloat16), max_new_tokens=32)\n",
    "    generated_texts = processor.batch_decode(output, skip_special_tokens=True)\n",
    "    print(\"model answer: \" + generated_texts[0].split(\"Assistant: \")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791930c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample_and_output(300, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c639b830",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "test_results = []\n",
    "\n",
    "for index in tqdm(range(0, len(dataset[\"test\"]), 10), desc=\"Generating predictions\"):\n",
    "    sample = dataset[\"test\"][index]\n",
    "\n",
    "    output = model.generate(\n",
    "        **test_dataset[index].to(\"cuda\", dtype=torch.bfloat16),\n",
    "        max_new_tokens=32\n",
    "    )\n",
    "    generated_texts = processor.batch_decode(output, skip_special_tokens=True)\n",
    "    pred = generated_texts[0].split(\"Assistant: \")[-1]\n",
    "\n",
    "    test_results.append({\n",
    "        \"scene_description\": sample[\"scene_description\"],\n",
    "        \"nuscenes_index\": sample[\"nuscenes_index\"],\n",
    "        \"task\": sample[\"task\"],\n",
    "        \"question\": sample[\"question\"],\n",
    "        \"answer\": sample[\"answer\"],\n",
    "        \"prediction\": pred\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d6e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "hf_dataset = Dataset.from_list(test_results)\n",
    "hf_dataset.push_to_hub(\n",
    "    \"MehdiJmlkh/SmolDriver-Results\",\n",
    "    private=False,\n",
    "    commit_message=\"Fine-tune 11000 steps\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
