{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5LvVwCX141",
        "outputId": "7924a237-a185-4bfd-9b40-b79dfbfc8b90"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.1/41.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m95.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.4/617.4 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m49.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m533.8/533.8 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.8/101.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.8/47.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.0/103.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for litellm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.5.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers num2words qformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UJ-gIhdDHXb"
      },
      "source": [
        "# Library code, modified"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mMQamS4ECwh0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def mask_top_right_quadrant(tensor):\n",
        "    \"\"\"\n",
        "    Masks the top right quadrant of a tensor.\n",
        "\n",
        "    Args:\n",
        "        tensor (Tensor): The input tensor.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: The masked tensor.\n",
        "    \"\"\"\n",
        "    rows, cols = tensor.shape[-2:]\n",
        "    mask = torch.ones(rows, cols).to(\"cuda:0\")\n",
        "    mask[: rows // 2, cols // 2 :] = 0\n",
        "    tensor=tensor.to(\"cuda:0\")\n",
        "    return tensor * mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "SmskOcPYCxq8"
      },
      "outputs": [],
      "source": [
        "from einops import rearrange, reduce\n",
        "from torch import Tensor, nn\n",
        "from zeta.nn import (\n",
        "    MultiQueryAttention,\n",
        "    SimpleFeedForward,\n",
        ")\n",
        "from zeta.nn.attention.cross_attention import CrossAttention\n",
        "\n",
        "\n",
        "def img_to_text(x: Tensor, seqlen: int, dim: int, norm: bool = True):\n",
        "    \"\"\"\n",
        "    Convert an image tensor to a text tensor.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input image tensor of shape (batch_size, channels, height, width).\n",
        "        seqlen (int): Length of the output text sequence.\n",
        "        dim (int): Dimension of the intermediate representation.\n",
        "        norm (bool, optional): Whether to apply layer normalization. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Output text tensor of shape (batch_size, seqlen, dim).\n",
        "\n",
        "    Example::\n",
        "        >>> x = torch.randn(2, 3, 32, 32)\n",
        "        >>> x = img_to_text(x, 100, 512)\n",
        "        >>> x.shape\n",
        "        torch.Size([2, 100, 512])\n",
        "    \"\"\"\n",
        "    b, c, h, w = x.shape\n",
        "\n",
        "    img = reduce(x, \"b c h w -> b c (h w)\", \"mean\").to(x.device)\n",
        "    img = nn.Linear(h * w, dim).to(device=x.device, dtype=x.dtype)(img)\n",
        "    img = rearrange(img, \"b c d -> b d c\")\n",
        "    img = nn.Linear(c, seqlen).to(device=x.device, dtype=x.dtype)(img)\n",
        "    img = rearrange(img, \"b d c -> b c d\")\n",
        "\n",
        "    if norm:\n",
        "        img = nn.LayerNorm(dim).to(device=x.device, dtype=x.dtype)(img)\n",
        "\n",
        "    return img\n",
        "\n",
        "\n",
        "class ImgBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ImgBlock is a module that performs multi-query attention, cross-attention, and feedforward operations on input tensors.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of the input tensors.\n",
        "        depth (int): The number of times the operations are applied.\n",
        "        heads (int): The number of attention heads.\n",
        "        dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
        "        emb_dropout (float, optional): The embedding dropout probability. Defaults to 0.1.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): The dimension of the input tensors.\n",
        "        depth (int): The number of times the operations are applied.\n",
        "        heads (int): The number of attention heads.\n",
        "        dropout (float): The dropout probability.\n",
        "        emb_dropout (float): The embedding dropout probability.\n",
        "        attn (MultiQueryAttention): The multi-query attention module.\n",
        "        cross_attn (CrossAttention): The cross-attention module.\n",
        "        feedforward (SimpleFeedForward): The feedforward module.\n",
        "\n",
        "    Methods:\n",
        "        forward(x: Tensor, img: Tensor) -> Tensor:\n",
        "            Performs the forward pass of the ImgBlock module.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        depth: int,\n",
        "        heads: int,\n",
        "        dropout: float = 0.1,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super(ImgBlock, self).__init__(*args, **kwargs)\n",
        "        self.dim = dim\n",
        "        self.depth = depth\n",
        "        self.heads = heads\n",
        "        self.dropout = dropout\n",
        "        self.attn = MultiQueryAttention(dim, heads)\n",
        "        self.cross_attn = CrossAttention(\n",
        "            dim=dim, heads=heads, dropout=dropout, *args, **kwargs\n",
        "        )\n",
        "\n",
        "        # Create a list of layers\n",
        "        self.self_attn_layers = nn.ModuleList([])\n",
        "        self.cross_attn_layers = nn.ModuleList([])\n",
        "        self.ffn_layers = nn.ModuleList([])\n",
        "\n",
        "        # Add the attn, cross attention, simple feedforward layers to the list\n",
        "        for _ in range(depth):\n",
        "            # Add the multi query attention layer\n",
        "            self.self_attn_layers.append(\n",
        "                MultiQueryAttention(dim, heads, *args, **kwargs)\n",
        "            )\n",
        "            # Add the cross attention layer\n",
        "            self.cross_attn_layers.append(\n",
        "                CrossAttention(\n",
        "                    dim=dim,\n",
        "                    heads=heads,\n",
        "                    dropout=dropout,\n",
        "                    *args,\n",
        "                    **kwargs,\n",
        "                )\n",
        "            )\n",
        "            # Add the simple feedforward layer\n",
        "            self.ffn_layers.append(\n",
        "                SimpleFeedForward(\n",
        "                    dim, dim * 4, dropout, *args, **kwargs\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor, img: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the ImgBlock module.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor.\n",
        "            img (Tensor): The image tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor after applying multi-query attention, cross-attention, and feedforward operations.\n",
        "\n",
        "        \"\"\"\n",
        "        b_t, s, d = x.shape\n",
        "        b, c, h, w = img.shape\n",
        "        device, dtype = x.device, x.dtype\n",
        "        img = img_to_text(img, s, d).to(device=x.device, dtype=x.dtype)\n",
        "\n",
        "        for self_attn, cross_attn, ffn in zip(\n",
        "            self.self_attn_layers,\n",
        "            self.cross_attn_layers,\n",
        "            self.ffn_layers,\n",
        "        ):\n",
        "            x, _, _ = self_attn(x)\n",
        "            x= x.to(device=device, dtype=dtype)\n",
        "            x = cross_attn(x, img).to(device=device, dtype=dtype)\n",
        "            x = ffn(x).to(device=device, dtype=dtype)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class TextBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    TextBlock module that performs self-attention and feedforward operations.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of the input and output tensors.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of layers in the module.\n",
        "        dropout (float, optional): The dropout probability. Defaults to 0.1.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): The dimension of the input and output tensors.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The number of layers in the module.\n",
        "        dropout (float): The dropout probability.\n",
        "        attn (MultiQueryAttention): The self-attention module.\n",
        "        feedforward (SimpleFeedForward): The feedforward module.\n",
        "        layers (nn.ModuleList): The list of layers in the module.\n",
        "\n",
        "    Methods:\n",
        "        forward(x: Tensor) -> Tensor:\n",
        "            Performs the forward pass of the TextBlock module.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        heads: int,\n",
        "        depth: int,\n",
        "        dropout: float = 0.1,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.depth = depth\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attn = MultiQueryAttention(dim, heads)\n",
        "        self.layers = nn.ModuleList([])\n",
        "        self.ffn_layers = nn.ModuleList([])\n",
        "\n",
        "        for _ in range(depth):\n",
        "            self.layers.append(\n",
        "                MultiQueryAttention(dim, heads, *args, **kwargs)\n",
        "            )\n",
        "\n",
        "            self.ffn_layers.append(\n",
        "                SimpleFeedForward(\n",
        "                    dim, dim * 4, dropout, *args, **kwargs\n",
        "                )\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Performs the forward pass of the TextBlock module.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor after self-attention and feedforward operations.\n",
        "\n",
        "        \"\"\"\n",
        "        for attn, ffn in zip(self.layers, self.ffn_layers):\n",
        "            x, _, _ = attn(x)\n",
        "            x = ffn(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2hi7UayDCsE2"
      },
      "outputs": [],
      "source": [
        "from torch import Tensor, nn\n",
        "\n",
        "# from qformer.blocks import ImgBlock, TextBlock\n",
        "# from qformer.masking import mask_top_right_quadrant\n",
        "\n",
        "\n",
        "class QFormer(nn.Module):\n",
        "    \"\"\"\n",
        "    QFormer is a transformer-based model for processing text and image inputs.\n",
        "\n",
        "    Args:\n",
        "        dim (int): The dimension of the model.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The depth of the model.\n",
        "        dropout (float, optional): The dropout rate. Defaults to 0.1.\n",
        "        text_block_depth (int, optional): The depth of the text block. Defaults to None.\n",
        "        img_text_block_depth (int, optional): The depth of the image text block. Defaults to None.\n",
        "\n",
        "    Attributes:\n",
        "        dim (int): The dimension of the model.\n",
        "        heads (int): The number of attention heads.\n",
        "        depth (int): The depth of the model.\n",
        "        dropout (float): The dropout rate.\n",
        "        img_block (ImgBlock): The image block of the model.\n",
        "        text_block (TextBlock): The text block of the model.\n",
        "        img_layers (nn.ModuleList): The list of image layers.\n",
        "        text_layers (nn.ModuleList): The list of text layers.\n",
        "\n",
        "    Examples:\n",
        "        >>> model = QFormer(dim=512, heads=8, depth=6, dropout=0.1, text_block_depth=2, img_text_block_depth=2)\n",
        "        >>> x = torch.randn(1, 10, 512)\n",
        "        >>> img = torch.randn(1, 3, 224, 224)\n",
        "        >>> out = model(x, img)\n",
        "        >>> out.shape\n",
        "        torch.Size([1, 10, 512])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        heads: int,\n",
        "        depth: int,\n",
        "        dropout: float = 0.1,\n",
        "        text_block_depth: int = None,\n",
        "        img_text_block_depth: int = None,\n",
        "        *args,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim = dim\n",
        "        self.heads = heads\n",
        "        self.depth = depth\n",
        "        self.dropout = dropout\n",
        "        self.img_block = ImgBlock(dim, depth, heads, dropout)\n",
        "        self.text_block = TextBlock(dim, heads, depth, dropout)\n",
        "        self.img_layers = nn.ModuleList([])\n",
        "        self.text_layers = nn.ModuleList([])\n",
        "\n",
        "        # Add the img and text layers to the list\n",
        "        for _ in range(depth):\n",
        "            self.img_layers.append(\n",
        "                ImgBlock(dim, img_text_block_depth, heads, dropout)\n",
        "            )\n",
        "            self.text_layers.append(\n",
        "                TextBlock(dim, heads, text_block_depth, dropout)\n",
        "            )\n",
        "\n",
        "    def forward(self, x: Tensor, img: Tensor) -> Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the QFormer model.\n",
        "\n",
        "        Args:\n",
        "            x (Tensor): The input tensor.\n",
        "            img (Tensor): The image tensor.\n",
        "\n",
        "        Returns:\n",
        "            Tensor: The output tensor.\n",
        "\n",
        "        \"\"\"\n",
        "        for text_block, img_block in zip(\n",
        "            self.text_layers, self.img_layers\n",
        "        ):\n",
        "            x = text_block(x) + x\n",
        "            x = mask_top_right_quadrant(x)\n",
        "            out = img_block(x, img).to(device=x.device, dtype=x.dtype) + x\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Moadel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "db6862c3988941599e1c642e64b6fb38",
            "3a8c6b486c0043e6890117b336fec8ee",
            "c16e3149c3954831a6c4dd5959984f4c",
            "038893a0b1e8415a8919e5378f8694fa",
            "b985590d0dba4d919e5432287f93f09e",
            "75ad6d80a3ed49d6929af21623530bf3",
            "0e31165d6bd5419ba40fdc4e634d9bc9",
            "fd36a2ed4e4742cdb2735b0a08fc348f",
            "dc44493ea5bf4f4c86322837454d0ff5",
            "ac188fc0504c46f28cbb1d23fc6ad5d7",
            "b6e8ebcde9214a88880d9cd71ed534a4"
          ]
        },
        "id": "2_Jb5sxG7opi",
        "outputId": "1b41b60c-8bc2-4129-ef7e-2b6154b419c2"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db6862c3988941599e1c642e64b6fb38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "from PIL import Image\n",
        "import num2words\n",
        "\n",
        "model_path = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": \"sample.png\"},\n",
        "            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    conversation,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "output_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "generated_texts = processor.batch_decode(output_ids, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lgL2EYagAbJ0",
        "outputId": "5caffff6-0531-4031-be41-af6d3a8ed584"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\"User:\\n\\n\\n\\n\\nDescribe this image.\\nAssistant: The image depicts a man with gray hair, wearing glasses, and dressed in a formal suit. The man's hair is short and neatly styled, and he has a well-groomed appearance. He is wearing a dark suit jacket over a light blue collared shirt. The background of the image is plain and white, which helps to focus attention on the man.\\n\\nThe man's expression is neutral, with a slight smile, suggesting a friendly or professional demeanor. His eyes are visible, and he appears to be looking directly at the camera, which is a common practice in professional photographs.\\n\\nThe image is a head\"]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generated_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMJ_GNbybv8V"
      },
      "source": [
        "# Q-Former"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Nsqamw6RgbQG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "# from qformer import QFormer\n",
        "\n",
        "class MultiQFormerVisionEncoder(nn.Module):\n",
        "    def __init__(self, qformer_config, num_qformers=4, output_dim=1152):\n",
        "        super().__init__()\n",
        "        self.qformers = nn.ModuleList([\n",
        "            QFormer(**qformer_config) for _ in range(num_qformers)\n",
        "        ])\n",
        "        # TODO: remove magic number 64\n",
        "        self.output_proj = nn.Linear(64 * num_qformers, num_qformers*output_dim*729)\n",
        "\n",
        "    def forward(self, x_list, img_list, mask_list=None):\n",
        "        \"\"\"\n",
        "        i used mean, it might be wrong and concating or using an extra merge layer might be the correct approach TODO later\n",
        "        \"\"\"\n",
        "        assert len(x_list) == len(img_list) == len(self.qformers)\n",
        "        if mask_list is None:\n",
        "            mask_list = [None] * len(img_list)\n",
        "\n",
        "        outputs = []\n",
        "        for i in range(len(self.qformers)):\n",
        "            # print(x_list[i])\n",
        "            # print(img_list[i])\n",
        "            out = self.qformers[i](x_list[i].to(\"cuda:0\"), img_list[i].to(\"cuda:0\")) # mask=mask_list[i] no mask\n",
        "            out = out.mean(dim=1)\n",
        "            outputs.append(out)\n",
        "\n",
        "        combined = torch.cat(outputs, dim=-1)\n",
        "        return self.output_proj(combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "yTG88t4KggRV"
      },
      "outputs": [],
      "source": [
        "qformer_config = {\n",
        "    \"dim\": 64, ####\n",
        "    \"heads\": 2,\n",
        "    \"depth\": 2,\n",
        "    \"dropout\": 0.1,\n",
        "    \"text_block_depth\": 2,\n",
        "    \"img_text_block_depth\": 2,\n",
        "}\n",
        "# - input_size: 512\n",
        "# - num_heads: 8\n",
        "# - num_layers: 8\n",
        "# - dropout: 0.1\n",
        "# - num_classes: 2\n",
        "# - num_patches: 2\n",
        "\n",
        "# vision_model = MultiQFormerVisionEncoder(qformer_config)\n",
        "\n",
        "# x_list = [torch.randn(1, 32, 512) for _ in range(4)]\n",
        "# img_list = [torch.randn(1, 3, 224, 224) for _ in range(4)]\n",
        "\n",
        "# output = vision_model(x_list, img_list)\n",
        "# print(output.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "agTGs5g8svZY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "\n",
        "class MultiCropVisionEncoder(nn.Module):\n",
        "    def __init__(self, qformer_config, num_qformers=4, output_dim=1152):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            qformer_config: config with at least `hidden_size` and `num_query_tokens` attributes\n",
        "            num_qformers: how many QFormers to use (typically 4 for 4-image processing)\n",
        "            output_dim: output dimension after projection\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.hidden_size = qformer_config[\"hidden_size\"]\n",
        "        self.num_query_tokens = qformer_config[\"num_query_tokens\"]\n",
        "        self.num_qformers = num_qformers\n",
        "\n",
        "        self.query_tokens = nn.Parameter(\n",
        "            torch.randn(self.num_query_tokens, self.hidden_size)\n",
        "        )\n",
        "\n",
        "        self.encoder = MultiQFormerVisionEncoder(\n",
        "            qformer_config=qformer_config,\n",
        "            num_qformers=num_qformers,\n",
        "            output_dim=output_dim\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values, patch_attention_mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            pixel_values: Tensor of shape (batch_size * 4, 3, H, W)\n",
        "            patch_attention_mask: Optional tensor of shape (batch_size * 4, H', W') or (batch_size * 4, num_patches)\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape (batch_size, 1, output_dim)\n",
        "        \"\"\"\n",
        "        b4, c, h, w = pixel_values.shape\n",
        "        print(pixel_values.shape)\n",
        "        assert b4 % self.num_qformers == 0, \"Input must be batch_size * num_qformers\"\n",
        "        batch_size = b4 // self.num_qformers\n",
        "\n",
        "        grouped_images = pixel_values.view(batch_size, self.num_qformers, c, h, w)\n",
        "        crop_list = list(grouped_images.unbind(dim=1))\n",
        "\n",
        "        if patch_attention_mask is not None:\n",
        "            grouped_masks = patch_attention_mask.view(batch_size, self.num_qformers, *patch_attention_mask.shape[1:])\n",
        "            mask_list = list(grouped_masks.unbind(dim=1))\n",
        "        else:\n",
        "            mask_list = [None] * self.num_qformers\n",
        "\n",
        "        query_embeds = self.query_tokens.unsqueeze(0).expand(batch_size, -1, -1).to(pixel_values.device)\n",
        "\n",
        "        x_list = [query_embeds for _ in range(self.num_qformers)]\n",
        "\n",
        "        out = self.encoder(x_list, crop_list, mask_list)\n",
        "        out = out.view(batch_size* 2, 729, 1152).to(device=pixel_values.device, dtype = torch.bfloat16)\n",
        "        print(out.shape)\n",
        "        return BaseModelOutput(last_hidden_state=out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P94Cs1S6kRbM"
      },
      "source": [
        "# Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## loading the test image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HhhaZqxMBREA"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "image = Image.open(\"sample.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "ures_eYHBWcc",
        "outputId": "4cf8b49f-8b22-45e0-9b4c-92c22012c000"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<PIL.Image.Image image mode=RGBA size=50x57>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "base_width = 50\n",
        "w_percent = (base_width / float(image.size[0]))\n",
        "h_size = int((float(image.size[1]) * float(w_percent)))\n",
        "resized_image = image.resize((base_width, h_size), Image.Resampling.LANCZOS)\n",
        "\n",
        "display(resized_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## modifying model config to support new vision model "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jngPNIp4saTs",
        "outputId": "db9bce76-9353-4eb5-d446-71c45643f5bc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dim': 1152,\n",
              " 'heads': 8,\n",
              " 'depth': 8,\n",
              " 'dropout': 0.1,\n",
              " 'text_block_depth': 2,\n",
              " 'img_text_block_depth': 2}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qformer_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "KmoPt3jotX8A"
      },
      "outputs": [],
      "source": [
        "old_config = model.config.vision_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLIhs6VW5Unw",
        "outputId": "ad699f63-c2ab-4997-a5d6-d317fe49c598"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SmolVLMVisionConfig {\n",
              "  \"attention_dropout\": 0.0,\n",
              "  \"hidden_act\": \"gelu_pytorch_tanh\",\n",
              "  \"hidden_size\": 1152,\n",
              "  \"image_size\": 384,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 4304,\n",
              "  \"layer_norm_eps\": 1e-06,\n",
              "  \"max_image_size\": {\n",
              "    \"longest_edge\": 384\n",
              "  },\n",
              "  \"model_type\": \"smolvlm_vision\",\n",
              "  \"num_attention_heads\": 16,\n",
              "  \"num_channels\": 3,\n",
              "  \"num_hidden_layers\": 27,\n",
              "  \"patch_size\": 14,\n",
              "  \"size\": {\n",
              "    \"longest_edge\": 1920\n",
              "  },\n",
              "  \"tie_word_embeddings\": false,\n",
              "  \"torch_dtype\": \"bfloat16\",\n",
              "  \"transformers_version\": \"4.54.1\",\n",
              "  \"use_base_siglip\": false\n",
              "}"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "old_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "xG8dLtqxrxkD"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    def __init__(self, dictionary, old_config):\n",
        "        for key, value in old_config.items():\n",
        "            setattr(self, key, value)\n",
        "        for key, value in dictionary.items():\n",
        "            setattr(self, key, value)\n",
        "\n",
        "qformer_config_obj = Config(qformer_config, old_config.get_config_dict(model_path)[0][\"vision_config\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEmWc2VjvvpI",
        "outputId": "1046749e-9aa4-47a9-ea7e-a879fa2024ac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qformer_config_obj.patch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "fNML8eee5cwR"
      },
      "outputs": [],
      "source": [
        "qformer_config[\"hidden_size\"] = 64\n",
        "qformer_config[\"num_query_tokens\"] = 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## swapping vision model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "the commented codes below are used to check the inner dimensions of the original and new model. not removed in case further debugging is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uv9JULIFYH2W"
      },
      "outputs": [],
      "source": [
        "old_vision_model = model.model.vision_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bM2tl7bSZdY",
        "outputId": "b95331a1-ccb5-477f-c8c9-c78310f50fc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape: torch.Size([4, 3, 100, 100])\n",
            "Output shape: torch.Size([4, 49, 1152])\n"
          ]
        }
      ],
      "source": [
        "# import torch\n",
        "\n",
        "# batch_size = 4\n",
        "# num_crops = 1\n",
        "# channels = 3\n",
        "# height = 100\n",
        "# width = 100\n",
        "\n",
        "# dummy_images = torch.randn(batch_size * num_crops, channels, height, width).to(device=\"cpu\", dtype=torch.bfloat16)\n",
        "\n",
        "# dummy_mask = None\n",
        "\n",
        "# with torch.no_grad():\n",
        "#     output = model.model.vision_model(dummy_images, patch_attention_mask=dummy_mask)\n",
        "\n",
        "# print(f\"Input shape: {dummy_images.shape}\")\n",
        "# print(f\"Output shape: {output.last_hidden_state.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1lhaWUNqKbF"
      },
      "outputs": [],
      "source": [
        "model.model.vision_model = MultiCropVisionEncoder(qformer_config, 2, output_dim=1152).to(\"cuda:0\")\n",
        "model.config.vision_config = qformer_config_obj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysWVthC_SGNm"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# batch_size = 2\n",
        "# num_crops = 1\n",
        "# channels = 3\n",
        "# height = 100\n",
        "# width = 100\n",
        "\n",
        "# dummy_images = torch.randn(batch_size * num_crops, channels, height, width).to(\"cpu\")\n",
        "\n",
        "# dummy_mask = None\n",
        "\n",
        "# vision_model = model.model.vision_model\n",
        "# with torch.no_grad():\n",
        "#     output = vision_model(dummy_images, patch_attention_mask=dummy_mask)\n",
        "\n",
        "# print(f\"Input shape: {dummy_images.shape}\")\n",
        "# print(f\"Output shape: {output.last_hidden_state.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twDstP3uqZyV"
      },
      "outputs": [],
      "source": [
        "# prompt = \"can you read this text?\"\n",
        "# imgs = [resized_image,resized_image,resized_image,resized_image]\n",
        "# inputs = processor(\n",
        "#     text=prompt,\n",
        "#     images=imgs,\n",
        "#     return_tensors=\"pt\"\n",
        "# ).to(model.device, torch.bfloat16)\n",
        "\n",
        "# generated_ids = model.generate(**inputs, max_new_tokens=50)\n",
        "# output = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "# print(\"SmolVLM Output:\", output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dummy = torch.randn(68, 3, 384,384).to(device=\"cuda:0\", dtype=torch.bfloat16)\n",
        "\n",
        "# model.model.vision_model(dummy).last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## testing new model inference for errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBqJkzmT7YcJ",
        "outputId": "feb2bd53-a943-4151-efd3-9c9d37471e0f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s71w_sAwq8ue",
        "outputId": "cff52de0-d9c1-4a85-9571-385da25eb829"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([34, 3, 384, 384])\n",
            "torch.Size([34, 729, 1152])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['User:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nwhat do you see in this image.\\nAssistant:']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": resized_image},\n",
        "            {\"type\": \"image\", \"url\": resized_image},\n",
        "            # {\"type\": \"image\", \"url\": resized_image},\n",
        "            # {\"type\": \"image\", \"url\": resized_image},\n",
        "            {\"type\": \"text\", \"text\": \"what do you see in this image.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    conversation,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "output_ids = model.generate(**inputs, max_new_tokens=16)\n",
        "generated_texts = processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "generated_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHB26t_qVYTz",
        "outputId": "a6bac142-933a-4905-c7e4-146c0abcce1e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "SmolVLMVisionTransformer(\n",
              "  (embeddings): SmolVLMVisionEmbeddings(\n",
              "    (patch_embedding): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14), padding=valid)\n",
              "    (position_embedding): Embedding(729, 1152)\n",
              "  )\n",
              "  (encoder): SmolVLMEncoder(\n",
              "    (layers): ModuleList(\n",
              "      (0-26): 27 x SmolVLMEncoderLayer(\n",
              "        (self_attn): SmolVLMVisionAttention(\n",
              "          (k_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (v_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (q_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "          (out_proj): Linear(in_features=1152, out_features=1152, bias=True)\n",
              "        )\n",
              "        (layer_norm1): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "        (mlp): SmolVLMVisionMLP(\n",
              "          (activation_fn): PytorchGELUTanh()\n",
              "          (fc1): Linear(in_features=1152, out_features=4304, bias=True)\n",
              "          (fc2): Linear(in_features=4304, out_features=1152, bias=True)\n",
              "        )\n",
              "        (layer_norm2): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (post_layernorm): LayerNorm((1152,), eps=1e-06, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.model.vision_model"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "75inOCkskmx7"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "038893a0b1e8415a8919e5378f8694fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac188fc0504c46f28cbb1d23fc6ad5d7",
            "placeholder": "​",
            "style": "IPY_MODEL_b6e8ebcde9214a88880d9cd71ed534a4",
            "value": " 2/2 [00:41&lt;00:00, 20.54s/it]"
          }
        },
        "03e27ec0c1dc4a71a500b6a1393f13b0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0e31165d6bd5419ba40fdc4e634d9bc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "169af40a42ca464e920ad9bf0e920956": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a8c6b486c0043e6890117b336fec8ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75ad6d80a3ed49d6929af21623530bf3",
            "placeholder": "​",
            "style": "IPY_MODEL_0e31165d6bd5419ba40fdc4e634d9bc9",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "3c32cda2cadd4520aed60fec4b672ad8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "41a9be4cd0dc4a1ea12c85439d437b64": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03e27ec0c1dc4a71a500b6a1393f13b0",
            "placeholder": "​",
            "style": "IPY_MODEL_3c32cda2cadd4520aed60fec4b672ad8",
            "value": " 1.29k/? [00:00&lt;00:00, 62.2kB/s]"
          }
        },
        "4257540f4b984f75bb668762859dee37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_82091e9a16924fa9a2951ef59ec64369",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9d693006d84f4e68898ea6b5682e7d04",
            "value": 1
          }
        },
        "4ae7eb3df49e4ab7beb86b97494a5ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_169af40a42ca464e920ad9bf0e920956",
            "placeholder": "​",
            "style": "IPY_MODEL_c11e9ff013484e5c9be9fdbc4753a863",
            "value": "config.json: "
          }
        },
        "4be6c7817ce043f89024199c1f4f1cef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75ad6d80a3ed49d6929af21623530bf3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82091e9a16924fa9a2951ef59ec64369": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "9d693006d84f4e68898ea6b5682e7d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac188fc0504c46f28cbb1d23fc6ad5d7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6e8ebcde9214a88880d9cd71ed534a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b985590d0dba4d919e5432287f93f09e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c11e9ff013484e5c9be9fdbc4753a863": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c16e3149c3954831a6c4dd5959984f4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd36a2ed4e4742cdb2735b0a08fc348f",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dc44493ea5bf4f4c86322837454d0ff5",
            "value": 2
          }
        },
        "d3c6e7d3115f4a9d9facd63fd0ab8b27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4ae7eb3df49e4ab7beb86b97494a5ffb",
              "IPY_MODEL_4257540f4b984f75bb668762859dee37",
              "IPY_MODEL_41a9be4cd0dc4a1ea12c85439d437b64"
            ],
            "layout": "IPY_MODEL_4be6c7817ce043f89024199c1f4f1cef"
          }
        },
        "db6862c3988941599e1c642e64b6fb38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a8c6b486c0043e6890117b336fec8ee",
              "IPY_MODEL_c16e3149c3954831a6c4dd5959984f4c",
              "IPY_MODEL_038893a0b1e8415a8919e5378f8694fa"
            ],
            "layout": "IPY_MODEL_b985590d0dba4d919e5432287f93f09e"
          }
        },
        "dc44493ea5bf4f4c86322837454d0ff5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fd36a2ed4e4742cdb2735b0a08fc348f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
