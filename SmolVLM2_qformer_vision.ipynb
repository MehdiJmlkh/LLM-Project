{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "je5LvVwCX141",
        "outputId": "1d4eaeba-bcf6-4799-d577-43ed0c1eaa45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m111.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q -U transformers==4.53.3 num2words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1UJ-gIhdDHXb"
      },
      "source": [
        "# Library code, modified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ped3NA2GQeb9"
      },
      "source": [
        "MQT Q-Former from https://github.com/gordonhu608/MQT-LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "utVwx1dYQeb9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.init import trunc_normal_\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def get_matry_n(num_visual_tokens):\n",
        "    if num_visual_tokens == 'first_stage':\n",
        "        return 729\n",
        "    elif num_visual_tokens == 'second_stage':\n",
        "        matry_list = range(1, 730, 4)\n",
        "        return random.choice(matry_list)\n",
        "\n",
        "    try:\n",
        "        num_visual_tokens = int(num_visual_tokens)\n",
        "        if 1 <= num_visual_tokens <= 729:\n",
        "            return num_visual_tokens\n",
        "    except (ValueError, TypeError):\n",
        "        print('The num_visual_tokens is should be an integer between 1 and 729')\n",
        "\n",
        "    raise ValueError(f\"Invalid input: {num_visual_tokens}\")\n",
        "\n",
        "def get_abs_pos(abs_pos, tgt_size):\n",
        "    # abs_pos: L, C\n",
        "    # tgt_size: (H, W)\n",
        "    # return: M, C\n",
        "    src_size = int(math.sqrt(abs_pos.size(0)))\n",
        "    # tgt_size = int(math.sqrt(tgt_size))\n",
        "    dtype = abs_pos.dtype\n",
        "    return F.interpolate(\n",
        "        abs_pos.float().reshape(1, src_size, src_size, -1).permute(0, 3, 1, 2),\n",
        "        size=(tgt_size[0], tgt_size[1]),\n",
        "        mode=\"bicubic\",\n",
        "        align_corners=False,\n",
        "    ).permute(0, 2, 3, 1).flatten(0, 2).to(dtype=dtype)\n",
        "\n",
        "\n",
        "# https://github.com/facebookresearch/mae/blob/efb2a8062c206524e35e47d04501ed4f544c0ae8/util/pos_embed.py#L20\n",
        "def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False):\n",
        "    \"\"\"\n",
        "    grid_size: int of the grid height and width\n",
        "    return:\n",
        "    pos_embed: [grid_size*grid_size, embed_dim] or [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)\n",
        "    \"\"\"\n",
        "    grid_h = np.arange(grid_size, dtype=np.float32)\n",
        "    grid_w = np.arange(grid_size, dtype=np.float32)\n",
        "    grid = np.meshgrid(grid_w, grid_h)  # here w goes first\n",
        "    grid = np.stack(grid, axis=0)\n",
        "\n",
        "    grid = grid.reshape([2, 1, grid_size, grid_size])\n",
        "\n",
        "\n",
        "    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)\n",
        "    if cls_token:\n",
        "        pos_embed = np.concatenate([np.zeros([1, embed_dim]), pos_embed], axis=0)\n",
        "    return pos_embed\n",
        "\n",
        "\n",
        "def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):\n",
        "    assert embed_dim % 2 == 0\n",
        "\n",
        "    # use half of dimensions to encode grid_h\n",
        "    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)\n",
        "    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_h, emb_w], axis=1)  # (H*W, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):\n",
        "    \"\"\"\n",
        "    embed_dim: output dimension for each position\n",
        "    pos: a list of positions to be encoded: size (M,)\n",
        "    out: (M, D)\n",
        "    \"\"\"\n",
        "    assert embed_dim % 2 == 0\n",
        "    omega = np.arange(embed_dim // 2, dtype=np.float32)\n",
        "    omega /= embed_dim / 2.\n",
        "    omega = 1. / 10000 ** omega  # (D/2,)\n",
        "\n",
        "    pos = pos.reshape(-1)  # (M,)\n",
        "    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), outer product\n",
        "\n",
        "    emb_sin = np.sin(out)  # (M, D/2)\n",
        "    emb_cos = np.cos(out)  # (M, D/2)\n",
        "\n",
        "    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)\n",
        "    return emb\n",
        "\n",
        "\n",
        "class Resampler(nn.Module):\n",
        "    \"\"\"\n",
        "    A 2D perceiver-resampler network with one cross attention layers by\n",
        "        (grid_size**2) learnable queries and 2d sincos pos_emb\n",
        "    Outputs:\n",
        "        A tensor with the shape of (grid_size**2, embed_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            grid_size,\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            kv_dim=None,\n",
        "            norm_layer=partial(nn.LayerNorm, eps=1e-6)\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.num_queries = grid_size ** 2\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.pos_embed = nn.Parameter(\n",
        "            torch.from_numpy(get_2d_sincos_pos_embed(kv_dim, grid_size)).to(torch.bfloat16)\n",
        "        ).requires_grad_(False)\n",
        "\n",
        "        self.query = nn.Parameter(torch.zeros(self.num_queries, kv_dim)).to(torch.bfloat16)\n",
        "        trunc_normal_(self.query, std=.02)\n",
        "\n",
        "        self.attn = nn.MultiheadAttention(kv_dim, num_heads).to(device = \"cuda:0\", dtype=torch.bfloat16)\n",
        "\n",
        "        self.ln_q = norm_layer(kv_dim).to(device = \"cuda:0\", dtype=torch.bfloat16)\n",
        "        self.ln_k = norm_layer(kv_dim).to(device = \"cuda:0\", dtype=torch.bfloat16)\n",
        "        self.ln_v = norm_layer(kv_dim).to(device = \"cuda:0\", dtype=torch.bfloat16)\n",
        "\n",
        "        # self.ln_post = norm_layer(kv_dim)\n",
        "        self.proj = nn.Parameter((embed_dim ** -0.5) * torch.randn(kv_dim, embed_dim)).to(device=\"cuda:0\", dtype=torch.bfloat16)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    def forward(self, x, num_visual_tokens=256, tgt_size=(24,24), attn_mask=None):\n",
        "        pos_embed = get_abs_pos(self.pos_embed, tgt_size)\n",
        "\n",
        "        x = x.permute(1, 0, 2)  # x: (seq_len, batch_size, dim)\n",
        "        B = x.shape[1]  # true batch size\n",
        "\n",
        "\n",
        "        matry_n = get_matry_n(num_visual_tokens)\n",
        "        q = self.query[:matry_n]  # (matry_n, dim)\n",
        "        q = self._repeat(q, B)    # (matry_n, B, dim)\n",
        "\n",
        "        k = self._repeat(pos_embed, B).to(device = \"cuda:0\", dtype=torch.bfloat16)\n",
        "        v = x\n",
        "        q= q.to(device = \"cuda:0\")\n",
        "        # self.pos_embed = self.pos_embed.to(device = \"cuda:0\")\n",
        "        # print(q.dtype)\n",
        "        # print(k.dtype)\n",
        "        # print(v.dtype)\n",
        "        # print (self.pos_embed.dtype)\n",
        "        q = self.ln_q(q + self.pos_embed[:matry_n].unsqueeze(1).to(device = \"cuda:0\")).to(device = x.device ,dtype=torch.bfloat16)\n",
        "        k = self.ln_k(k).to(device = x.device ,dtype=torch.bfloat16)\n",
        "        v = self.ln_v(v).to(device = x.device ,dtype=torch.bfloat16)\n",
        "\n",
        "        out = self.attn(q, k, v, attn_mask=attn_mask)[0]  # (matry_n, B, dim)\n",
        "\n",
        "\n",
        "        x = out.permute(1, 0, 2)\n",
        "\n",
        "        x = x @ self.proj\n",
        "        # print(x.shape)\n",
        "        return x\n",
        "\n",
        "    def _repeat(self, query, N: int):\n",
        "        return query.unsqueeze(1).repeat(1, N, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mMQamS4ECwh0"
      },
      "outputs": [],
      "source": [
        "class MultiResampler(nn.Module):\n",
        "    \"\"\"\n",
        "    experimental multi-resampler\n",
        "\n",
        "    Args:\n",
        "        num_resamplers:int = number of resamplers(qformers)\n",
        "\n",
        "    Outputs:\n",
        "        A tensor with the shape of (grid_size**2, embed_dim)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            grid_size,\n",
        "            embed_dim,\n",
        "            num_heads,\n",
        "            num_resamplers,\n",
        "            kv_dim=None,\n",
        "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.resamplers = [Resampler(grid_size, embed_dim, num_heads, kv_dim, norm_layer) for _ in range(num_resamplers)]\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        for i in  range(len(self.resamplers)):\n",
        "            self.resamplers[i]._init_weights(m)\n",
        "\n",
        "    def forward(self, x, num_visual_tokens=256, tgt_size=(27,27), attn_mask=None):\n",
        "        # raise NotImplementedError()\n",
        "        # x should be a list\n",
        "        assert len(x) == len(self.resamplers)\n",
        "\n",
        "        num_vt_each = num_visual_tokens//len(self.resamplers)\n",
        "        outs = []\n",
        "        for i in range(len(self.resamplers)):\n",
        "            outs.append(self.resamplers[i].forward(x[i], num_vt_each, tgt_size))\n",
        "        concat = torch.cat(outs, dim=1)\n",
        "        return concat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQE97ZPO27U4"
      },
      "source": [
        "# Load Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190,
          "referenced_widgets": [
            "3c7bcc95bb044a0f89271c7f3ff11c9f",
            "f28df3f6c3154669a2a5ee1b686e4f7c",
            "5b154a85b3f64e39a0af2841581be05c",
            "99d9d75c533c483ab903af2d2a07dc0b",
            "33ce3c17ec3d4956a9a30c53f1bcb9e7",
            "cdd69539ed1042c394fc1cf6e3d82ddb",
            "cab01bcbc7174d96841119c5902308df",
            "e4bda96e1be54449aec540e23593972a",
            "ed6349a4404c4990849c5bab62069150",
            "773baea910884fe1bafb49f4bfbccec1",
            "d79ac994c7b44144bb5ecdb71997372b"
          ]
        },
        "id": "2_Jb5sxG7opi",
        "outputId": "b3f7e99c-b8f0-4251-cc2f-83a432bb8a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c7bcc95bb044a0f89271c7f3ff11c9f"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "from PIL import Image\n",
        "import num2words\n",
        "\n",
        "model_path = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_path,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"cuda\",\n",
        ")\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJM0ZhM081hc",
        "outputId": "4a3a8924-afa7-4246-87f8-91f0095eb2b2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2246784880"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Total trainable params\n",
        "sum(p.numel() for p in model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHtfLrpK8-0M",
        "outputId": "47d0a01c-2b00-42eb-c718-229d8b7fef05"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "412987248"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vision trainable params\n",
        "sum(p.numel() for p in model.model.vision_model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtIhZ1h327U5"
      },
      "outputs": [],
      "source": [
        "# conversation = [\n",
        "#     {\n",
        "#         \"role\": \"user\",\n",
        "#         \"content\": [\n",
        "#             {\"type\": \"image\", \"url\": \"sample.png\"},\n",
        "#             {\"type\": \"text\", \"text\": \"Describe this image.\"}\n",
        "#         ]\n",
        "#     }\n",
        "# ]\n",
        "\n",
        "# inputs = processor.apply_chat_template(\n",
        "#     conversation,\n",
        "#     add_generation_prompt=True,\n",
        "#     tokenize=True,\n",
        "#     return_dict=True,\n",
        "#     return_tensors=\"pt\"\n",
        "# ).to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "# output_ids = model.generate(**inputs, max_new_tokens=128)\n",
        "# generated_texts = processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "# generated_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMJ_GNbybv8V"
      },
      "source": [
        "# Q-Former"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yTG88t4KggRV"
      },
      "outputs": [],
      "source": [
        "qformer_config = {\n",
        "    \"grid_size\": 27,\n",
        "    \"embed_dim\": 1152,\n",
        "    \"num_heads\": 16,\n",
        "    \"num_resamplers\": 4,\n",
        "    \"kv_dim\": 1152\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "agTGs5g8svZY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import ViTModel, AutoImageProcessor\n",
        "from transformers.modeling_outputs import BaseModelOutput\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from functools import partial\n",
        "from torchvision.transforms import functional as F_transforms\n",
        "\n",
        "class MultiCropVisionEncoder(nn.Module):\n",
        "    def __init__(self, qformer_config, vision_tower):\n",
        "        super().__init__()\n",
        "        self.num_resamplers = qformer_config[\"num_resamplers\"]\n",
        "        self.embed_dim = qformer_config[\"embed_dim\"]\n",
        "        self.grid_size = qformer_config[\"grid_size\"]\n",
        "\n",
        "        self.vision_tower = vision_tower\n",
        "        vision_hidden_size = self.vision_tower.config.hidden_size\n",
        "        self.vision_tower_image_size = self.vision_tower.config.image_size\n",
        "\n",
        "        self.vision_proj = nn.Linear(vision_hidden_size, self.embed_dim)\n",
        "\n",
        "        self.multi_resampler = MultiResampler(\n",
        "            grid_size=qformer_config[\"grid_size\"],\n",
        "            embed_dim=qformer_config[\"embed_dim\"],\n",
        "            num_heads=qformer_config[\"num_heads\"],\n",
        "            num_resamplers=qformer_config[\"num_resamplers\"],\n",
        "            kv_dim=qformer_config[\"kv_dim\"],\n",
        "            norm_layer=partial(nn.LayerNorm, eps=1e-6),\n",
        "        )\n",
        "\n",
        "    def forward(self, pixel_values, patch_attention_mask=None):\n",
        "        pixel_values = pixel_values.to(torch.bfloat16)\n",
        "        b4, c, h, w = pixel_values.shape\n",
        "        assert b4 % self.num_resamplers == 0, \"Batch size must be divisible by number of resamplers\"\n",
        "        batch_size = b4 // self.num_resamplers\n",
        "\n",
        "        # image = np.transpose(pixel_values[0].cpu().numpy(), (1, 2, 0))\n",
        "        # plt.imshow(image / 255.0)\n",
        "        # plt.show()\n",
        "\n",
        "        patch_embeddings_list = []\n",
        "        for i in range(self.num_resamplers):\n",
        "            start = i * batch_size\n",
        "            end = (i + 1) * batch_size\n",
        "            sub_batch = pixel_values[start:end]\n",
        "\n",
        "            resized_sub_batch = F_transforms.resize(sub_batch, size=(432, 432))\n",
        "\n",
        "\n",
        "            vision_outputs = self.vision_tower(pixel_values=resized_sub_batch, interpolate_pos_encoding=True)\n",
        "            embeddings = vision_outputs.last_hidden_state\n",
        "\n",
        "            embeddings = embeddings.to(torch.bfloat16)\n",
        "\n",
        "            embeddings = embeddings[:, 1:, :]\n",
        "            embeddings = self.vision_proj(embeddings)\n",
        "\n",
        "            patch_embeddings_list.append(embeddings)\n",
        "\n",
        "        patch_embeddings = torch.stack(patch_embeddings_list, dim=1)\n",
        "\n",
        "        # print(patch_embeddings.shape)\n",
        "        x_list = list(patch_embeddings.unbind(dim=1))\n",
        "        out = self.multi_resampler(x_list, num_visual_tokens=self.grid_size**2 * self.num_resamplers)\n",
        "\n",
        "        out = out.view(batch_size * self.num_resamplers, self.grid_size**2, self.embed_dim).to(\n",
        "            device=pixel_values.device,\n",
        "            dtype=torch.bfloat16\n",
        "        )\n",
        "        # print(out.shape)\n",
        "        return BaseModelOutput(last_hidden_state=out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIG6QyPtr_pI"
      },
      "source": [
        "## Test Vision Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "k9m73v4CDAmA"
      },
      "outputs": [],
      "source": [
        "vision_tower = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\").to(device= \"cuda:0\", dtype=torch.bfloat16)\n",
        "test_vm = MultiCropVisionEncoder(qformer_config, vision_tower)\n",
        "test_vm = test_vm.to(device= \"cuda\", dtype=torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18v_CgKGDMEY",
        "outputId": "6494b4cb-6a56-4d27-ebaa-fdb0cb71fa49"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 729, 1152])\n"
          ]
        }
      ],
      "source": [
        "dummy_image = torch.randn(4, 3, 384, 384).to(device=\"cuda\", dtype=torch.bfloat16)\n",
        "output = test_vm(dummy_image)\n",
        "print(output.last_hidden_state.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBsSfFsTSy26",
        "outputId": "4aedf076-a7fc-45bf-e3db-274569e453b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87275136"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sum(p.numel() for p in test_vm.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P94Cs1S6kRbM"
      },
      "source": [
        "# Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vRuxwI927VA"
      },
      "source": [
        "## loading the test image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HhhaZqxMBREA"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "image = Image.open(\"sample.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ures_eYHBWcc",
        "outputId": "f93f2abb-0fd2-4d2d-c5eb-879203761751"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGBA size=50x57>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "base_width = 50\n",
        "w_percent = (base_width / float(image.size[0]))\n",
        "h_size = int((float(image.size[1]) * float(w_percent)))\n",
        "resized_image = image.resize((base_width, h_size), Image.Resampling.LANCZOS)\n",
        "\n",
        "display(resized_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFmfTfvs27VB"
      },
      "source": [
        "## modifying model config to support new vision model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "KmoPt3jotX8A"
      },
      "outputs": [],
      "source": [
        "old_config = model.config.vision_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xG8dLtqxrxkD"
      },
      "outputs": [],
      "source": [
        "from transformers import PretrainedConfig\n",
        "\n",
        "class Config(PretrainedConfig):\n",
        "    model_type = \"smolvlm_vision\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        self.attention_dropout = 0.0\n",
        "        self.hidden_act = \"gelu_pytorch_tanh\"\n",
        "        self.hidden_size = 1152\n",
        "        self.image_size = 384\n",
        "        self.initializer_range = 0.02\n",
        "        self.intermediate_size = 4304\n",
        "        self.layer_norm_eps = 1e-06\n",
        "        self.max_image_size = {\"longest_edge\": 384}\n",
        "        self.num_attention_heads = 16\n",
        "        self.num_channels = 3\n",
        "        self.num_hidden_layers = 27\n",
        "        self.patch_size = 14\n",
        "        self.size = {\"longest_edge\": 1920}\n",
        "        self.tie_word_embeddings = False\n",
        "        self.torch_dtype = \"bfloat16\"\n",
        "        self.transformers_version = \"4.55.0\"\n",
        "        self.use_base_siglip = False\n",
        "\n",
        "        self.grid_size = 27\n",
        "        self.embed_dim = 1152\n",
        "        self.num_heads = 16\n",
        "        self.num_resamplers = 4\n",
        "        self.kv_dim = 1152\n",
        "\n",
        "        super().__init__(**kwargs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "koa5P0EU27VE"
      },
      "source": [
        "## swapping vision model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLgieA7I27VE"
      },
      "source": [
        "the commented codes below are used to check the inner dimensions of the original and new model. not removed in case further debugging is needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Uv9JULIFYH2W"
      },
      "outputs": [],
      "source": [
        "old_vision_model = model.model.vision_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpAWs_1AAm_R"
      },
      "outputs": [],
      "source": [
        "model.model.vision_model = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1lhaWUNqKbF"
      },
      "outputs": [],
      "source": [
        "vision_tower = ViTModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "vision_tower.embeddings.patch_embeddings = vision_tower.embeddings.patch_embeddings.to(torch.bfloat16)\n",
        "model.model.vision_model = MultiCropVisionEncoder(qformer_config, vision_tower).to(device=model.device, dtype=torch.bfloat16)\n",
        "model.config.vision_config = Config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hocaetLZ-OpO",
        "outputId": "2cf2bf9d-f28e-4840-bd46-56f906821698"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "87275136"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sum(p.numel() for p in model.model.vision_model.parameters() if p.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFx6698S27VH",
        "outputId": "7ce72e69-71f4-42be-a593-55d726c880a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 729, 1152])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dummy = torch.randn(1, 3, 384,384).to(device=\"cpu\", dtype=torch.bfloat16)\n",
        "# old_vision_model = old_vision_model.to(\"cpu\")\n",
        "# old_vision_model(dummy).last_hidden_state.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "VVoR99N-NsBj"
      },
      "outputs": [],
      "source": [
        "processor.image_processor.max_image_size[\"longest_edge\"]= 384\n",
        "processor.image_processor.do_image_splitting=False\n",
        "processor.image_processor.do_resize=True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6yYimRX27VI"
      },
      "source": [
        "## testing new model inference for errors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s71w_sAwq8ue",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b293472b-df46-408b-873d-3486c5b2a81d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['User:\\n\\n\\n\\n\\ndescribe this image.\\nAssistant: The image depicts a man with gray hair and glasses. He is wearing a dark suit jacket over a light blue shirt. The man is smiling and looking directly at']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": resized_image},\n",
        "            # {\"type\": \"image\", \"url\": resized_image},\n",
        "            # {\"type\": \"image\", \"url\": resized_image},\n",
        "            # {\"type\": \"image\", \"url\": resized_image},\n",
        "            {\"type\": \"text\", \"text\": \"describe this image.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "inputs = processor.apply_chat_template(\n",
        "    conversation,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ").to(model.device, dtype=torch.bfloat16)\n",
        "\n",
        "output_ids = model.generate(**inputs, max_new_tokens=32)\n",
        "generated_texts = processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "generated_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHB26t_qVYTz",
        "outputId": "53d175d5-94e0-4c24-bc2d-4db2806d351e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "MultiCropVisionEncoder(\n",
              "  (patch_embed): Conv2d(3, 1152, kernel_size=(14, 14), stride=(14, 14))\n",
              "  (multi_resampler): MultiResampler()\n",
              ")"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.model.vision_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FJDxjOkNphWx"
      },
      "source": [
        "# Save in huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gidCtG7pqELZ"
      },
      "outputs": [],
      "source": [
        "!pip install -q huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "20f25380ee794d76979431fca2119f4a",
            "ba2c144d571644949fc837d52609158b",
            "f2b9dba3dcfb49b6a09089574910c81e",
            "ba2cb862e3194148a39f00bfda1ae250",
            "17d0e7ec58774511987afe49ec6d50d5",
            "726c3d3f9168429bbfe9e8ab180670f1",
            "1f9bf4f1d2fc409daf603d1443fcae8c",
            "02696e48c9bb4248951a69c6217c60a9",
            "754e72a679c649ef98244ef131cf1542",
            "151db2128dc34c48b4a783c39c0c7217",
            "e7d5dfeffe1d4827bdc8cb45d5fbf0ad",
            "37f02bfa0f0e497aa114289c0f76ebcc",
            "a751916db355418f93fd18ff92991732",
            "b43a2e1aa8044bd1b0377797aacd660c",
            "0da1b0547d9045d19e6690896b60c2ee",
            "6b5bcc414f1e40bcbd6a6278aff16a0b",
            "41cda6da2a914bafa0f7bd37b9aa7758",
            "08fef7d868db4189a62e7d9337165596",
            "bff783e277784b64a2d1c3d2f12dec56",
            "764d3a79203d4a02bbcaaa35b45334fc"
          ]
        },
        "id": "UDMP8UEYrnnM",
        "outputId": "90a2bc3c-9b97-4776-ad98-999535ef2a51"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20f25380ee794d76979431fca2119f4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "from huggingface_hub import notebook_login, create_repo, upload_folder\n",
        "import os\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YXmXjQUW5jMb"
      },
      "outputs": [],
      "source": [
        "model = model.to(\"cuda\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gtLKN_Ynpk7r",
        "outputId": "265131e4-00fa-488f-8d31-aa2d960bfc81"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['./SmolDriver/processor_config.json']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model_name = \"SmolDriver\"\n",
        "hf_username = \"ArianFiroozi\"\n",
        "repo_id = f\"{hf_username}/{model_name}\"\n",
        "save_directory = f\"./{model_name}\"\n",
        "\n",
        "model.save_pretrained(save_directory)\n",
        "model.config.save_pretrained(save_directory)\n",
        "processor.save_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240,
          "referenced_widgets": [
            "0facc7c5303c49618dbb42a9e7caba2d",
            "b029240e1d3642ecaed0039cdfe337de",
            "9199aa9a9c6a4357a6e94ceaf3f70a73",
            "cfcc0eaf2ae442449880b470f5bc6731",
            "406de73247cd4d2a99ea75d16488e21c",
            "01b26307ddb5426d836b81ad1db1d6fb",
            "4623cb361652463bbc900760350aecb9",
            "df0ee92b485e4d38bdae6539e85660ae",
            "00a461e3da51427983d3bf7a26a46830",
            "4d2a9e70fe7f490693c3681d975e0cdc",
            "ddd7599ca6ee4992ad534af6d2ff2518",
            "e5c18c2634ec4f40992f9fc5e3c36b7d",
            "578b069ff11c4102a3f5db126f8270ca",
            "20f06fa9167e4e20b6ddc3d463e92aa7",
            "b5ccfa445d544c758784e859d70b38ea",
            "580dded13a154476882fce7d0423ff54",
            "eedd9d8a521345eabfb31b8257375672",
            "19b2049706e449fbaf25be7b61d7a572",
            "c0dc8f0a19c342638b0a03f1a42ae88a",
            "1a9e90b901624bae855af6e0a6255c2d",
            "9c1a973a85ff4ed9a90e2e84cdf80aaa",
            "0253805b2e5a46839aa96bd4224c1a27"
          ]
        },
        "id": "cw0_2z65-CqS",
        "outputId": "b36a4b52-e4ca-4ef1-f40c-3bfccab7e2a0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0facc7c5303c49618dbb42a9e7caba2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/89.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5c18c2634ec4f40992f9fc5e3c36b7d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/4.02G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/ArianFiroozi/SmolDriver/commit/1f00ddda9f01861975e81a002c9fa74988d60bdb', commit_message='Upload SmolVLMForConditionalGeneration', commit_description='', oid='1f00ddda9f01861975e81a002c9fa74988d60bdb', pr_url=None, repo_url=RepoUrl('https://huggingface.co/ArianFiroozi/SmolDriver', endpoint='https://huggingface.co', repo_type='model', repo_id='ArianFiroozi/SmolDriver'), pr_revision=None, pr_num=None)"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import AutoModelForImageTextToText, AutoProcessor\n",
        "\n",
        "model.push_to_hub(\"ArianFiroozi/SmolDriver\", safe_serialization=True)\n",
        "# processor.save_pretrained(\"SmolDriver\", push_to_hub=True, repo_id=\"ArianFiroozi/SmolDriver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dz7Qt5at_8n",
        "outputId": "54373a9e-2d01-4504-ca15-ce0c6c514de7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at ArianFiroozi/SmolDriver were not used when initializing SmolVLMForConditionalGeneration: ['model.vision_model.vision_proj.bias', 'model.vision_model.vision_proj.weight', 'model.vision_model.vision_tower.embeddings.cls_token', 'model.vision_model.vision_tower.embeddings.patch_embeddings.projection.bias', 'model.vision_model.vision_tower.embeddings.patch_embeddings.projection.weight', 'model.vision_model.vision_tower.embeddings.position_embeddings', 'model.vision_model.vision_tower.encoder.layer.0.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.0.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.0.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.0.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.0.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.0.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.0.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.0.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.0.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.0.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.0.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.0.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.0.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.0.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.0.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.0.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.1.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.1.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.1.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.1.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.1.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.1.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.1.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.1.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.1.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.1.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.1.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.1.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.1.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.1.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.1.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.1.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.10.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.10.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.10.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.10.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.10.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.10.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.10.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.10.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.10.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.10.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.10.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.10.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.10.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.10.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.10.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.10.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.11.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.11.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.11.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.11.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.11.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.11.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.11.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.11.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.11.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.11.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.11.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.11.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.11.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.11.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.11.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.11.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.2.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.2.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.2.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.2.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.2.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.2.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.2.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.2.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.2.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.2.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.2.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.2.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.2.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.2.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.2.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.2.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.3.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.3.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.3.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.3.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.3.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.3.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.3.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.3.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.3.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.3.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.3.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.3.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.3.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.3.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.3.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.3.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.4.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.4.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.4.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.4.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.4.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.4.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.4.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.4.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.4.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.4.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.4.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.4.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.4.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.4.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.4.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.4.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.5.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.5.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.5.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.5.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.5.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.5.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.5.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.5.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.5.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.5.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.5.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.5.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.5.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.5.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.5.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.5.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.6.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.6.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.6.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.6.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.6.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.6.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.6.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.6.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.6.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.6.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.6.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.6.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.6.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.6.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.6.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.6.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.7.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.7.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.7.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.7.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.7.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.7.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.7.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.7.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.7.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.7.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.7.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.7.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.7.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.7.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.7.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.7.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.8.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.8.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.8.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.8.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.8.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.8.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.8.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.8.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.8.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.8.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.8.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.8.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.8.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.8.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.8.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.8.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.9.attention.attention.key.bias', 'model.vision_model.vision_tower.encoder.layer.9.attention.attention.key.weight', 'model.vision_model.vision_tower.encoder.layer.9.attention.attention.query.bias', 'model.vision_model.vision_tower.encoder.layer.9.attention.attention.query.weight', 'model.vision_model.vision_tower.encoder.layer.9.attention.attention.value.bias', 'model.vision_model.vision_tower.encoder.layer.9.attention.attention.value.weight', 'model.vision_model.vision_tower.encoder.layer.9.attention.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.9.attention.output.dense.weight', 'model.vision_model.vision_tower.encoder.layer.9.intermediate.dense.bias', 'model.vision_model.vision_tower.encoder.layer.9.intermediate.dense.weight', 'model.vision_model.vision_tower.encoder.layer.9.layernorm_after.bias', 'model.vision_model.vision_tower.encoder.layer.9.layernorm_after.weight', 'model.vision_model.vision_tower.encoder.layer.9.layernorm_before.bias', 'model.vision_model.vision_tower.encoder.layer.9.layernorm_before.weight', 'model.vision_model.vision_tower.encoder.layer.9.output.dense.bias', 'model.vision_model.vision_tower.encoder.layer.9.output.dense.weight', 'model.vision_model.vision_tower.layernorm.bias', 'model.vision_model.vision_tower.layernorm.weight', 'model.vision_model.vision_tower.pooler.dense.bias', 'model.vision_model.vision_tower.pooler.dense.weight']\n",
            "- This IS expected if you are initializing SmolVLMForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SmolVLMForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SmolVLMForConditionalGeneration were not initialized from the model checkpoint at ArianFiroozi/SmolDriver and are newly initialized: ['model.vision_model.embeddings.patch_embedding.bias', 'model.vision_model.embeddings.patch_embedding.weight', 'model.vision_model.embeddings.position_embedding.weight', 'model.vision_model.encoder.layers.0.layer_norm1.bias', 'model.vision_model.encoder.layers.0.layer_norm1.weight', 'model.vision_model.encoder.layers.0.layer_norm2.bias', 'model.vision_model.encoder.layers.0.layer_norm2.weight', 'model.vision_model.encoder.layers.0.mlp.fc1.bias', 'model.vision_model.encoder.layers.0.mlp.fc1.weight', 'model.vision_model.encoder.layers.0.mlp.fc2.bias', 'model.vision_model.encoder.layers.0.mlp.fc2.weight', 'model.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.1.layer_norm1.bias', 'model.vision_model.encoder.layers.1.layer_norm1.weight', 'model.vision_model.encoder.layers.1.layer_norm2.bias', 'model.vision_model.encoder.layers.1.layer_norm2.weight', 'model.vision_model.encoder.layers.1.mlp.fc1.bias', 'model.vision_model.encoder.layers.1.mlp.fc1.weight', 'model.vision_model.encoder.layers.1.mlp.fc2.bias', 'model.vision_model.encoder.layers.1.mlp.fc2.weight', 'model.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.10.layer_norm1.bias', 'model.vision_model.encoder.layers.10.layer_norm1.weight', 'model.vision_model.encoder.layers.10.layer_norm2.bias', 'model.vision_model.encoder.layers.10.layer_norm2.weight', 'model.vision_model.encoder.layers.10.mlp.fc1.bias', 'model.vision_model.encoder.layers.10.mlp.fc1.weight', 'model.vision_model.encoder.layers.10.mlp.fc2.bias', 'model.vision_model.encoder.layers.10.mlp.fc2.weight', 'model.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.11.layer_norm1.bias', 'model.vision_model.encoder.layers.11.layer_norm1.weight', 'model.vision_model.encoder.layers.11.layer_norm2.bias', 'model.vision_model.encoder.layers.11.layer_norm2.weight', 'model.vision_model.encoder.layers.11.mlp.fc1.bias', 'model.vision_model.encoder.layers.11.mlp.fc1.weight', 'model.vision_model.encoder.layers.11.mlp.fc2.bias', 'model.vision_model.encoder.layers.11.mlp.fc2.weight', 'model.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.12.layer_norm1.bias', 'model.vision_model.encoder.layers.12.layer_norm1.weight', 'model.vision_model.encoder.layers.12.layer_norm2.bias', 'model.vision_model.encoder.layers.12.layer_norm2.weight', 'model.vision_model.encoder.layers.12.mlp.fc1.bias', 'model.vision_model.encoder.layers.12.mlp.fc1.weight', 'model.vision_model.encoder.layers.12.mlp.fc2.bias', 'model.vision_model.encoder.layers.12.mlp.fc2.weight', 'model.vision_model.encoder.layers.12.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.12.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.12.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.12.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.12.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.12.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.12.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.12.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.13.layer_norm1.bias', 'model.vision_model.encoder.layers.13.layer_norm1.weight', 'model.vision_model.encoder.layers.13.layer_norm2.bias', 'model.vision_model.encoder.layers.13.layer_norm2.weight', 'model.vision_model.encoder.layers.13.mlp.fc1.bias', 'model.vision_model.encoder.layers.13.mlp.fc1.weight', 'model.vision_model.encoder.layers.13.mlp.fc2.bias', 'model.vision_model.encoder.layers.13.mlp.fc2.weight', 'model.vision_model.encoder.layers.13.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.13.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.13.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.13.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.13.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.13.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.13.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.13.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.14.layer_norm1.bias', 'model.vision_model.encoder.layers.14.layer_norm1.weight', 'model.vision_model.encoder.layers.14.layer_norm2.bias', 'model.vision_model.encoder.layers.14.layer_norm2.weight', 'model.vision_model.encoder.layers.14.mlp.fc1.bias', 'model.vision_model.encoder.layers.14.mlp.fc1.weight', 'model.vision_model.encoder.layers.14.mlp.fc2.bias', 'model.vision_model.encoder.layers.14.mlp.fc2.weight', 'model.vision_model.encoder.layers.14.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.14.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.14.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.14.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.14.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.14.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.14.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.14.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.15.layer_norm1.bias', 'model.vision_model.encoder.layers.15.layer_norm1.weight', 'model.vision_model.encoder.layers.15.layer_norm2.bias', 'model.vision_model.encoder.layers.15.layer_norm2.weight', 'model.vision_model.encoder.layers.15.mlp.fc1.bias', 'model.vision_model.encoder.layers.15.mlp.fc1.weight', 'model.vision_model.encoder.layers.15.mlp.fc2.bias', 'model.vision_model.encoder.layers.15.mlp.fc2.weight', 'model.vision_model.encoder.layers.15.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.15.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.15.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.15.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.15.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.15.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.15.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.15.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.16.layer_norm1.bias', 'model.vision_model.encoder.layers.16.layer_norm1.weight', 'model.vision_model.encoder.layers.16.layer_norm2.bias', 'model.vision_model.encoder.layers.16.layer_norm2.weight', 'model.vision_model.encoder.layers.16.mlp.fc1.bias', 'model.vision_model.encoder.layers.16.mlp.fc1.weight', 'model.vision_model.encoder.layers.16.mlp.fc2.bias', 'model.vision_model.encoder.layers.16.mlp.fc2.weight', 'model.vision_model.encoder.layers.16.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.16.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.16.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.16.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.16.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.16.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.16.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.16.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.17.layer_norm1.bias', 'model.vision_model.encoder.layers.17.layer_norm1.weight', 'model.vision_model.encoder.layers.17.layer_norm2.bias', 'model.vision_model.encoder.layers.17.layer_norm2.weight', 'model.vision_model.encoder.layers.17.mlp.fc1.bias', 'model.vision_model.encoder.layers.17.mlp.fc1.weight', 'model.vision_model.encoder.layers.17.mlp.fc2.bias', 'model.vision_model.encoder.layers.17.mlp.fc2.weight', 'model.vision_model.encoder.layers.17.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.17.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.17.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.17.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.17.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.17.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.17.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.17.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.18.layer_norm1.bias', 'model.vision_model.encoder.layers.18.layer_norm1.weight', 'model.vision_model.encoder.layers.18.layer_norm2.bias', 'model.vision_model.encoder.layers.18.layer_norm2.weight', 'model.vision_model.encoder.layers.18.mlp.fc1.bias', 'model.vision_model.encoder.layers.18.mlp.fc1.weight', 'model.vision_model.encoder.layers.18.mlp.fc2.bias', 'model.vision_model.encoder.layers.18.mlp.fc2.weight', 'model.vision_model.encoder.layers.18.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.18.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.18.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.18.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.18.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.18.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.18.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.18.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.19.layer_norm1.bias', 'model.vision_model.encoder.layers.19.layer_norm1.weight', 'model.vision_model.encoder.layers.19.layer_norm2.bias', 'model.vision_model.encoder.layers.19.layer_norm2.weight', 'model.vision_model.encoder.layers.19.mlp.fc1.bias', 'model.vision_model.encoder.layers.19.mlp.fc1.weight', 'model.vision_model.encoder.layers.19.mlp.fc2.bias', 'model.vision_model.encoder.layers.19.mlp.fc2.weight', 'model.vision_model.encoder.layers.19.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.19.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.19.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.19.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.19.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.19.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.19.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.19.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.2.layer_norm1.bias', 'model.vision_model.encoder.layers.2.layer_norm1.weight', 'model.vision_model.encoder.layers.2.layer_norm2.bias', 'model.vision_model.encoder.layers.2.layer_norm2.weight', 'model.vision_model.encoder.layers.2.mlp.fc1.bias', 'model.vision_model.encoder.layers.2.mlp.fc1.weight', 'model.vision_model.encoder.layers.2.mlp.fc2.bias', 'model.vision_model.encoder.layers.2.mlp.fc2.weight', 'model.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.20.layer_norm1.bias', 'model.vision_model.encoder.layers.20.layer_norm1.weight', 'model.vision_model.encoder.layers.20.layer_norm2.bias', 'model.vision_model.encoder.layers.20.layer_norm2.weight', 'model.vision_model.encoder.layers.20.mlp.fc1.bias', 'model.vision_model.encoder.layers.20.mlp.fc1.weight', 'model.vision_model.encoder.layers.20.mlp.fc2.bias', 'model.vision_model.encoder.layers.20.mlp.fc2.weight', 'model.vision_model.encoder.layers.20.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.20.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.20.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.20.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.20.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.20.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.20.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.20.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.21.layer_norm1.bias', 'model.vision_model.encoder.layers.21.layer_norm1.weight', 'model.vision_model.encoder.layers.21.layer_norm2.bias', 'model.vision_model.encoder.layers.21.layer_norm2.weight', 'model.vision_model.encoder.layers.21.mlp.fc1.bias', 'model.vision_model.encoder.layers.21.mlp.fc1.weight', 'model.vision_model.encoder.layers.21.mlp.fc2.bias', 'model.vision_model.encoder.layers.21.mlp.fc2.weight', 'model.vision_model.encoder.layers.21.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.21.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.21.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.21.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.21.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.21.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.21.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.21.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.22.layer_norm1.bias', 'model.vision_model.encoder.layers.22.layer_norm1.weight', 'model.vision_model.encoder.layers.22.layer_norm2.bias', 'model.vision_model.encoder.layers.22.layer_norm2.weight', 'model.vision_model.encoder.layers.22.mlp.fc1.bias', 'model.vision_model.encoder.layers.22.mlp.fc1.weight', 'model.vision_model.encoder.layers.22.mlp.fc2.bias', 'model.vision_model.encoder.layers.22.mlp.fc2.weight', 'model.vision_model.encoder.layers.22.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.22.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.22.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.22.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.22.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.22.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.22.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.22.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.23.layer_norm1.bias', 'model.vision_model.encoder.layers.23.layer_norm1.weight', 'model.vision_model.encoder.layers.23.layer_norm2.bias', 'model.vision_model.encoder.layers.23.layer_norm2.weight', 'model.vision_model.encoder.layers.23.mlp.fc1.bias', 'model.vision_model.encoder.layers.23.mlp.fc1.weight', 'model.vision_model.encoder.layers.23.mlp.fc2.bias', 'model.vision_model.encoder.layers.23.mlp.fc2.weight', 'model.vision_model.encoder.layers.23.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.23.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.23.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.23.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.23.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.23.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.23.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.23.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.24.layer_norm1.bias', 'model.vision_model.encoder.layers.24.layer_norm1.weight', 'model.vision_model.encoder.layers.24.layer_norm2.bias', 'model.vision_model.encoder.layers.24.layer_norm2.weight', 'model.vision_model.encoder.layers.24.mlp.fc1.bias', 'model.vision_model.encoder.layers.24.mlp.fc1.weight', 'model.vision_model.encoder.layers.24.mlp.fc2.bias', 'model.vision_model.encoder.layers.24.mlp.fc2.weight', 'model.vision_model.encoder.layers.24.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.24.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.24.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.24.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.24.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.24.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.24.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.24.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.25.layer_norm1.bias', 'model.vision_model.encoder.layers.25.layer_norm1.weight', 'model.vision_model.encoder.layers.25.layer_norm2.bias', 'model.vision_model.encoder.layers.25.layer_norm2.weight', 'model.vision_model.encoder.layers.25.mlp.fc1.bias', 'model.vision_model.encoder.layers.25.mlp.fc1.weight', 'model.vision_model.encoder.layers.25.mlp.fc2.bias', 'model.vision_model.encoder.layers.25.mlp.fc2.weight', 'model.vision_model.encoder.layers.25.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.25.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.25.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.25.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.25.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.25.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.25.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.25.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.26.layer_norm1.bias', 'model.vision_model.encoder.layers.26.layer_norm1.weight', 'model.vision_model.encoder.layers.26.layer_norm2.bias', 'model.vision_model.encoder.layers.26.layer_norm2.weight', 'model.vision_model.encoder.layers.26.mlp.fc1.bias', 'model.vision_model.encoder.layers.26.mlp.fc1.weight', 'model.vision_model.encoder.layers.26.mlp.fc2.bias', 'model.vision_model.encoder.layers.26.mlp.fc2.weight', 'model.vision_model.encoder.layers.26.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.26.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.26.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.26.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.26.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.26.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.26.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.26.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.3.layer_norm1.bias', 'model.vision_model.encoder.layers.3.layer_norm1.weight', 'model.vision_model.encoder.layers.3.layer_norm2.bias', 'model.vision_model.encoder.layers.3.layer_norm2.weight', 'model.vision_model.encoder.layers.3.mlp.fc1.bias', 'model.vision_model.encoder.layers.3.mlp.fc1.weight', 'model.vision_model.encoder.layers.3.mlp.fc2.bias', 'model.vision_model.encoder.layers.3.mlp.fc2.weight', 'model.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.4.layer_norm1.bias', 'model.vision_model.encoder.layers.4.layer_norm1.weight', 'model.vision_model.encoder.layers.4.layer_norm2.bias', 'model.vision_model.encoder.layers.4.layer_norm2.weight', 'model.vision_model.encoder.layers.4.mlp.fc1.bias', 'model.vision_model.encoder.layers.4.mlp.fc1.weight', 'model.vision_model.encoder.layers.4.mlp.fc2.bias', 'model.vision_model.encoder.layers.4.mlp.fc2.weight', 'model.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.5.layer_norm1.bias', 'model.vision_model.encoder.layers.5.layer_norm1.weight', 'model.vision_model.encoder.layers.5.layer_norm2.bias', 'model.vision_model.encoder.layers.5.layer_norm2.weight', 'model.vision_model.encoder.layers.5.mlp.fc1.bias', 'model.vision_model.encoder.layers.5.mlp.fc1.weight', 'model.vision_model.encoder.layers.5.mlp.fc2.bias', 'model.vision_model.encoder.layers.5.mlp.fc2.weight', 'model.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.6.layer_norm1.bias', 'model.vision_model.encoder.layers.6.layer_norm1.weight', 'model.vision_model.encoder.layers.6.layer_norm2.bias', 'model.vision_model.encoder.layers.6.layer_norm2.weight', 'model.vision_model.encoder.layers.6.mlp.fc1.bias', 'model.vision_model.encoder.layers.6.mlp.fc1.weight', 'model.vision_model.encoder.layers.6.mlp.fc2.bias', 'model.vision_model.encoder.layers.6.mlp.fc2.weight', 'model.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.7.layer_norm1.bias', 'model.vision_model.encoder.layers.7.layer_norm1.weight', 'model.vision_model.encoder.layers.7.layer_norm2.bias', 'model.vision_model.encoder.layers.7.layer_norm2.weight', 'model.vision_model.encoder.layers.7.mlp.fc1.bias', 'model.vision_model.encoder.layers.7.mlp.fc1.weight', 'model.vision_model.encoder.layers.7.mlp.fc2.bias', 'model.vision_model.encoder.layers.7.mlp.fc2.weight', 'model.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.8.layer_norm1.bias', 'model.vision_model.encoder.layers.8.layer_norm1.weight', 'model.vision_model.encoder.layers.8.layer_norm2.bias', 'model.vision_model.encoder.layers.8.layer_norm2.weight', 'model.vision_model.encoder.layers.8.mlp.fc1.bias', 'model.vision_model.encoder.layers.8.mlp.fc1.weight', 'model.vision_model.encoder.layers.8.mlp.fc2.bias', 'model.vision_model.encoder.layers.8.mlp.fc2.weight', 'model.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'model.vision_model.encoder.layers.9.layer_norm1.bias', 'model.vision_model.encoder.layers.9.layer_norm1.weight', 'model.vision_model.encoder.layers.9.layer_norm2.bias', 'model.vision_model.encoder.layers.9.layer_norm2.weight', 'model.vision_model.encoder.layers.9.mlp.fc1.bias', 'model.vision_model.encoder.layers.9.mlp.fc1.weight', 'model.vision_model.encoder.layers.9.mlp.fc2.bias', 'model.vision_model.encoder.layers.9.mlp.fc2.weight', 'model.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'model.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'model.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'model.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'model.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'model.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'model.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'model.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'model.vision_model.post_layernorm.bias', 'model.vision_model.post_layernorm.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "\n",
        "test_model = AutoModelForImageTextToText.from_pretrained(\"ArianFiroozi/SmolDriver\",\n",
        "                                                         torch_dtype=torch.bfloat16,\n",
        "                                                         device_map=\"cuda\")\n",
        "test_processor = AutoProcessor.from_pretrained(\"ArianFiroozi/SmolDriver\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2ziNd_7BkaI"
      },
      "outputs": [],
      "source": [
        "test_model.model.vision_model = test_model.model.vision_model.to(torch.bfloat16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "lYKqFxcmuJQ-",
        "outputId": "7ac7aed7-924f-4464-8183-120ae167aa11"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Expected bias to have type Float but got BFloat16",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-312588056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0moutput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_new_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mgenerated_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mgenerated_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m             \u001b[0;31m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2634\u001b[0;31m             result = self._sample(\n\u001b[0m\u001b[1;32m   2635\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2636\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3614\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_prefill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3615\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3616\u001b[0m                 \u001b[0mis_prefill\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3617\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/smolvlm/modeling_smolvlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, labels, use_cache, output_attentions, output_hidden_states, cache_position, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    951\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    957\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/smolvlm/modeling_smolvlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_attention_mask, image_hidden_states, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpixel_values\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0mimage_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpixel_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mimage_hidden_states\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m             \u001b[0mimage_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/smolvlm/modeling_smolvlm.py\u001b[0m in \u001b[0;36mget_image_features\u001b[0;34m(self, pixel_values, pixel_attention_mask)\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;31m# Get sequence from the vision encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m         \u001b[0mimage_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvision_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m         \u001b[0mimage_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_hidden_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/smolvlm/modeling_smolvlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, patch_attention_mask, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    443\u001b[0m             \u001b[0mpatch_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0mpatch_attention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_attention_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/smolvlm/modeling_smolvlm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pixel_values, patch_attention_mask)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_im_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_im_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpixel_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mpatch_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpixel_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpatch_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected bias to have type Float but got BFloat16"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "conversation = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\", \"url\": resized_image},\n",
        "            {\"type\": \"image\", \"url\": resized_image},\n",
        "            {\"type\": \"image\", \"url\": resized_image},\n",
        "            {\"type\": \"image\", \"url\": resized_image},\n",
        "            {\"type\": \"text\", \"text\": \"describe this image.\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "inputs = test_processor.apply_chat_template(\n",
        "    conversation,\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_dict=True,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Move all input tensors to the model's device\n",
        "inputs = {k: v.to(test_model.device) if isinstance(v, torch.Tensor) else v for k, v in inputs.items()}\n",
        "\n",
        "output_ids = test_model.generate(**inputs, max_new_tokens=32)\n",
        "generated_texts = test_processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "generated_texts"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# knowledge transfer to vision model"
      ],
      "metadata": {
        "id": "7gl5CQb3tMl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "in_channels = 3\n",
        "out_channels = 1\n",
        "input_shape = (3, 384, 384)\n",
        "output_shape = (1, 729, 1152)\n",
        "\n",
        "epochs = 3\n",
        "batch_size = 4\n",
        "num_samples = 1000\n",
        "lr = 1e-3\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3Nzo7dU1tStN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MockImageDataset(Dataset):\n",
        "    def __init__(self, n_samples, in_shape, out_shape):\n",
        "        self.n = n_samples\n",
        "        self.in_shape = in_shape\n",
        "        self.out_shape = out_shape\n",
        "        self.data = torch.randn(n_samples, *in_shape)\n",
        "        self.labels = torch.randn(n_samples, *out_shape)\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "TAoRwP05tkHo"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "teacher = model.model.vision_model\n",
        "student = test_vm"
      ],
      "metadata": {
        "id": "TDkKZ1QhuROe"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for p in teacher.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "print(f\"Teacher params: {sum(p.numel() for p in teacher.parameters()):,}\")\n",
        "print(f\"Student params: {sum(p.numel() for p in student.parameters()):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-0H50lqtulw",
        "outputId": "95c1002b-ed2b-4021-cefa-4472b4bfa893"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Teacher params: 412,987,248\n",
            "Student params: 87,275,136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MockImageDataset(num_samples, input_shape, output_shape)\n",
        "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "optimizer = torch.optim.Adam(student.parameters(), lr=lr)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "teacher.eval()\n",
        "for epoch in range(epochs):\n",
        "    student.train()\n",
        "    total_loss = 0.0\n",
        "    for xb, _ in loader:\n",
        "        xb = xb.to(\"cuda\", dtype=torch.bfloat16)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            teacher_out = teacher(xb)\n",
        "\n",
        "        # xb_stud = xb.to(device, dtype=torch.bfloat16)\n",
        "\n",
        "        student_out = student(xb)\n",
        "\n",
        "        loss = criterion(student_out.last_hidden_state, teacher_out.last_hidden_state)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * xb.size(0)\n",
        "\n",
        "    n = len(dataset)\n",
        "    print(f\"ep {epoch}/{epochs}, student loss: {total_loss/n:.6f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mn4T3_y9tfrx",
        "outputId": "efef6e3c-c1ae-4fb9-e612-4cf98fb9ec07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ep 0/3, student loss: 0.325781\n",
            "ep 1/3, student loss: 0.166207\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "WQE97ZPO27U4",
        "9vRuxwI927VA",
        "koa5P0EU27VE"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (conda)",
      "language": "python",
      "name": "base"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "00a461e3da51427983d3bf7a26a46830": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01b26307ddb5426d836b81ad1db1d6fb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0253805b2e5a46839aa96bd4224c1a27": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "02696e48c9bb4248951a69c6217c60a9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08fef7d868db4189a62e7d9337165596": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bff783e277784b64a2d1c3d2f12dec56",
            "placeholder": "​",
            "style": "IPY_MODEL_764d3a79203d4a02bbcaaa35b45334fc",
            "value": "Connecting..."
          }
        },
        "0da1b0547d9045d19e6690896b60c2ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "0facc7c5303c49618dbb42a9e7caba2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b029240e1d3642ecaed0039cdfe337de",
              "IPY_MODEL_9199aa9a9c6a4357a6e94ceaf3f70a73",
              "IPY_MODEL_cfcc0eaf2ae442449880b470f5bc6731"
            ],
            "layout": "IPY_MODEL_406de73247cd4d2a99ea75d16488e21c"
          }
        },
        "151db2128dc34c48b4a783c39c0c7217": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17d0e7ec58774511987afe49ec6d50d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_b43a2e1aa8044bd1b0377797aacd660c",
            "style": "IPY_MODEL_0da1b0547d9045d19e6690896b60c2ee",
            "tooltip": ""
          }
        },
        "19b2049706e449fbaf25be7b61d7a572": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a9e90b901624bae855af6e0a6255c2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1f9bf4f1d2fc409daf603d1443fcae8c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "20f06fa9167e4e20b6ddc3d463e92aa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0dc8f0a19c342638b0a03f1a42ae88a",
            "max": 4016752712,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a9e90b901624bae855af6e0a6255c2d",
            "value": 4016752712
          }
        },
        "20f25380ee794d76979431fca2119f4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_1f9bf4f1d2fc409daf603d1443fcae8c"
          }
        },
        "37f02bfa0f0e497aa114289c0f76ebcc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "406de73247cd4d2a99ea75d16488e21c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41cda6da2a914bafa0f7bd37b9aa7758": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4623cb361652463bbc900760350aecb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4d2a9e70fe7f490693c3681d975e0cdc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "578b069ff11c4102a3f5db126f8270ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eedd9d8a521345eabfb31b8257375672",
            "placeholder": "​",
            "style": "IPY_MODEL_19b2049706e449fbaf25be7b61d7a572",
            "value": "model.safetensors: 100%"
          }
        },
        "580dded13a154476882fce7d0423ff54": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b5bcc414f1e40bcbd6a6278aff16a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "726c3d3f9168429bbfe9e8ab180670f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b5bcc414f1e40bcbd6a6278aff16a0b",
            "placeholder": "​",
            "style": "IPY_MODEL_41cda6da2a914bafa0f7bd37b9aa7758",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "754e72a679c649ef98244ef131cf1542": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "764d3a79203d4a02bbcaaa35b45334fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9199aa9a9c6a4357a6e94ceaf3f70a73": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df0ee92b485e4d38bdae6539e85660ae",
            "max": 89,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_00a461e3da51427983d3bf7a26a46830",
            "value": 89
          }
        },
        "9c1a973a85ff4ed9a90e2e84cdf80aaa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a751916db355418f93fd18ff92991732": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b029240e1d3642ecaed0039cdfe337de": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01b26307ddb5426d836b81ad1db1d6fb",
            "placeholder": "​",
            "style": "IPY_MODEL_4623cb361652463bbc900760350aecb9",
            "value": "README.md: 100%"
          }
        },
        "b43a2e1aa8044bd1b0377797aacd660c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5ccfa445d544c758784e859d70b38ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c1a973a85ff4ed9a90e2e84cdf80aaa",
            "placeholder": "​",
            "style": "IPY_MODEL_0253805b2e5a46839aa96bd4224c1a27",
            "value": " 4.02G/4.02G [01:40&lt;00:00, 52.8MB/s]"
          }
        },
        "ba2c144d571644949fc837d52609158b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_02696e48c9bb4248951a69c6217c60a9",
            "placeholder": "​",
            "style": "IPY_MODEL_754e72a679c649ef98244ef131cf1542",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "ba2cb862e3194148a39f00bfda1ae250": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "CheckboxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_37f02bfa0f0e497aa114289c0f76ebcc",
            "style": "IPY_MODEL_a751916db355418f93fd18ff92991732",
            "value": true
          }
        },
        "bff783e277784b64a2d1c3d2f12dec56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0dc8f0a19c342638b0a03f1a42ae88a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfcc0eaf2ae442449880b470f5bc6731": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d2a9e70fe7f490693c3681d975e0cdc",
            "placeholder": "​",
            "style": "IPY_MODEL_ddd7599ca6ee4992ad534af6d2ff2518",
            "value": " 89.0/89.0 [00:00&lt;00:00, 2.72kB/s]"
          }
        },
        "ddd7599ca6ee4992ad534af6d2ff2518": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df0ee92b485e4d38bdae6539e85660ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c18c2634ec4f40992f9fc5e3c36b7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_578b069ff11c4102a3f5db126f8270ca",
              "IPY_MODEL_20f06fa9167e4e20b6ddc3d463e92aa7",
              "IPY_MODEL_b5ccfa445d544c758784e859d70b38ea"
            ],
            "layout": "IPY_MODEL_580dded13a154476882fce7d0423ff54"
          }
        },
        "e7d5dfeffe1d4827bdc8cb45d5fbf0ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eedd9d8a521345eabfb31b8257375672": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2b9dba3dcfb49b6a09089574910c81e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "PasswordModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_151db2128dc34c48b4a783c39c0c7217",
            "placeholder": "​",
            "style": "IPY_MODEL_e7d5dfeffe1d4827bdc8cb45d5fbf0ad",
            "value": ""
          }
        },
        "3c7bcc95bb044a0f89271c7f3ff11c9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f28df3f6c3154669a2a5ee1b686e4f7c",
              "IPY_MODEL_5b154a85b3f64e39a0af2841581be05c",
              "IPY_MODEL_99d9d75c533c483ab903af2d2a07dc0b"
            ],
            "layout": "IPY_MODEL_33ce3c17ec3d4956a9a30c53f1bcb9e7"
          }
        },
        "f28df3f6c3154669a2a5ee1b686e4f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdd69539ed1042c394fc1cf6e3d82ddb",
            "placeholder": "​",
            "style": "IPY_MODEL_cab01bcbc7174d96841119c5902308df",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "5b154a85b3f64e39a0af2841581be05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4bda96e1be54449aec540e23593972a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ed6349a4404c4990849c5bab62069150",
            "value": 2
          }
        },
        "99d9d75c533c483ab903af2d2a07dc0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_773baea910884fe1bafb49f4bfbccec1",
            "placeholder": "​",
            "style": "IPY_MODEL_d79ac994c7b44144bb5ecdb71997372b",
            "value": " 2/2 [00:38&lt;00:00, 19.00s/it]"
          }
        },
        "33ce3c17ec3d4956a9a30c53f1bcb9e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd69539ed1042c394fc1cf6e3d82ddb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cab01bcbc7174d96841119c5902308df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4bda96e1be54449aec540e23593972a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed6349a4404c4990849c5bab62069150": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "773baea910884fe1bafb49f4bfbccec1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d79ac994c7b44144bb5ecdb71997372b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}