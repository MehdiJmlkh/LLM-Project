{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17800a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install num2words evaluate bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4459a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "from PIL import Image\n",
    "import num2words\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import ViTModel, AutoImageProcessor\n",
    "from transformers.modeling_outputs import BaseModelOutput\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "from torchvision.transforms import functional as F_transforms\n",
    "from huggingface_hub import PyTorchModelHubMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d920742",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"HuggingFaceTB/SmolVLM2-2.2B-Instruct\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda\",\n",
    ")\n",
    "\n",
    "processor.image_processor.max_image_size[\"longest_edge\"]= 384\n",
    "processor.image_processor.do_image_splitting=False\n",
    "processor.image_processor.do_resize=True\n",
    "\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6208d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vision_encoder import MultiCropVisionEncoder\n",
    "\n",
    "vision_tower = ViTModel.from_pretrained(\"MehdiJmlkh/SmolDriverVisionTower-FT\").to(device= \"cuda:0\", dtype=torch.bfloat16)\n",
    "vision_tower.embeddings.patch_embeddings = vision_tower.embeddings.patch_embeddings.to(torch.bfloat16)\n",
    "vision_model = MultiCropVisionEncoder.from_pretrained(\"MehdiJmlkh/SmolDriverVision-FT\", vision_tower = vision_tower).to(\"cuda\", torch.bfloat16)\n",
    "\n",
    "old_vision_model = model.model.vision_model\n",
    "model.model.vision_model = vision_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cb727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import PeftModel\n",
    "\n",
    "# peft_config = LoraConfig(\n",
    "#     r=16,\n",
    "#     lora_alpha=16,\n",
    "#     target_modules=\"all-linear\",\n",
    "#     lora_dropout=0.05,\n",
    "#     task_type=\"CAUSAL_LM\",\n",
    "# )\n",
    "\n",
    "# peft_model = get_peft_model(model, peft_config)\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, \"MehdiJmlkh/SmolDriver-Peft\", is_trainable=True)\n",
    "\n",
    "for param in peft_model.model.model.vision_model.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d2f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json\n",
    "from datasets import Dataset\n",
    "\n",
    "drive_lm = load_dataset(\"MehdiJmlkh/DriveLM\")\n",
    "nuscenes = load_dataset(\"MehdiJmlkh/nuscenes\")\n",
    "\n",
    "class DriveLM:\n",
    "    def __init__(self, drive_lm, nuscenes):\n",
    "        self.drive_lm = drive_lm\n",
    "        self.nuscenes = nuscenes\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "      sample = self.drive_lm[idx]\n",
    "      nuscenes_index = sample[\"nuscenes_index\"]\n",
    "      sample['images'] = self.nuscenes[nuscenes_index]\n",
    "\n",
    "      return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.drive_lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3d4e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = {split: DriveLM(drive_lm[split].select(range(1000)), nuscenes[split]) for split in [\"test\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd81fbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class VLMQATestDataset(Dataset):\n",
    "    def __init__(self, dataset: DriveLM, is_train=True):\n",
    "        self.dataset = dataset\n",
    "        self.is_train = is_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.dataset[idx]\n",
    "\n",
    "        user_msg = []\n",
    "        cameras = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']\n",
    "        for camera in cameras:\n",
    "            user_msg.extend([\n",
    "                {\"type\": \"text\", \"text\": camera},\n",
    "                {\"type\": \"image\", \"image\": item[\"images\"][camera]}\n",
    "            ])\n",
    "\n",
    "        user_msg.append({\n",
    "            \"type\": \"text\",\n",
    "            \"text\":f\"Scene description:{item['scene_description']} Question: {item['question']}\"\n",
    "        })\n",
    "\n",
    "        assistant_msg = [{\"type\": \"text\", \"text\": item[\"answer\"]}]\n",
    "\n",
    "        if not self.is_train:\n",
    "            conversation = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": user_msg\n",
    "                }\n",
    "            ]\n",
    "            return self.__apply_chat_template(conversation, True)\n",
    "\n",
    "\n",
    "        conversation = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_msg\n",
    "            }\n",
    "        ]\n",
    "        inputs = self.__apply_chat_template(conversation)\n",
    "\n",
    "        label_start_idx = self.__get_label_start_idx(inputs)\n",
    "\n",
    "        labels = inputs['input_ids'].clone()\n",
    "        labels[:, :label_start_idx] = -100\n",
    "        inputs['labels'] = labels\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def __apply_chat_template(self, conversation, add_generation_prompt=False):\n",
    "        return processor.apply_chat_template(\n",
    "            conversation,\n",
    "            add_generation_prompt=add_generation_prompt,\n",
    "            tokenize=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "    def __get_label_start_idx(self, inputs):\n",
    "        utterance_id = processor.tokenizer.convert_tokens_to_ids('<end_of_utterance>')\n",
    "        utterance_idx = inputs['input_ids'][0].tolist().index(utterance_id)\n",
    "        num_assistant_ids = 4\n",
    "        label_idx = utterance_idx + num_assistant_ids + 1\n",
    "\n",
    "        return label_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f215acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = VLMQATestDataset(test_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d1a38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "index = 0\n",
    "\n",
    "def display_sample_and_output(index, model):\n",
    "    sample = test_dataset[\"test\"][index]\n",
    "\n",
    "    def display_image(image, title):\n",
    "        plt.imshow(image)\n",
    "        plt.axis('off')\n",
    "        plt.title(title)\n",
    "\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    cameras = ['CAM_FRONT_LEFT', 'CAM_FRONT', 'CAM_FRONT_RIGHT', 'CAM_BACK_LEFT', 'CAM_BACK', 'CAM_BACK_RIGHT']\n",
    "    for i, camera in enumerate(cameras):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "        display_image(sample['images'][camera], camera)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    for key, value in sample.items():\n",
    "        if type(value) is str:\n",
    "            print(f\"{key}: {value}\")\n",
    "\n",
    "    output = model.generate(**test_ds[index].to(\"cuda\", dtype=torch.bfloat16), max_new_tokens=32)\n",
    "    generated_texts = processor.batch_decode(output, skip_special_tokens=True)\n",
    "    print(\"model answer: \" + generated_texts[0].split(\"Assistant: \")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1caf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample_and_output(0, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa9989",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "refs=[]\n",
    "preds=[]\n",
    "\n",
    "for index in range(0, len(test_ds), 10):\n",
    "    sample = test_dataset[\"test\"][index]\n",
    "\n",
    "    refs.append(sample['answer'])\n",
    "\n",
    "\n",
    "    output = model.generate(**test_ds[index].to(\"cuda\", dtype=torch.bfloat16), max_new_tokens=32)\n",
    "    generated_texts = processor.batch_decode(output, skip_special_tokens=True)\n",
    "    preds.append(generated_texts[0].split(\"Assistant: \")[-1])\n",
    "    print(\".\", end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a27102",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "bleu_result = bleu.compute(predictions=preds, references=refs)\n",
    "\n",
    "bert_result = bertscore.compute(predictions=preds, references=refs, lang=\"en\")\n",
    "\n",
    "scores = {\n",
    "      \"BLEU\": bleu_result[\"bleu\"],\n",
    "      \"BERTScore_P\": sum(bert_result[\"precision\"]) / len(bert_result[\"precision\"]),\n",
    "      \"BERTScore_R\": sum(bert_result[\"recall\"]) / len(bert_result[\"recall\"]),\n",
    "      \"BERTScore_F1\": sum(bert_result[\"f1\"]) / len(bert_result[\"f1\"])\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
